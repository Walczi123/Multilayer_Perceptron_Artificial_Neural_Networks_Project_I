Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.32063041231278794;0.10547725619660155
10;0.2856799347646086;0.10510665374960504
15;0.31398066052634716;0.10909236884118373
20;0.22106634526269556;0.10616923129935078
25;0.3308126623529084;0.10605279182365397
30;0.2771361030600423;0.10098348560689142
35;0.26832321437933837;0.10248066972872957
40;0.26517143707239654;0.10149342589174652
45;0.3307511419410442;0.10798557605878781
50;0.3398105691912937;0.10716759935099522
55;0.2925870597385476;0.10425748184932031
60;0.28016865412089237;0.10149326261108421
65;0.25284748094627085;0.10438465836936517
70;0.3209117265965581;0.10347173059313801
75;0.29168668093888983;0.10473734583881636
80;0.2908476743472118;0.10612971264072785
85;0.27761690539923456;0.10073671856017911
90;0.29302463083121805;0.10377019747162768
95;0.34551413749878607;0.11043239128897991
100;0.2772867645432349;0.10043366391792596
105;0.40146780948699623;0.11239504416769197
110;0.29634978864233513;0.10312462921974141
115;0.3038460799106844;0.10649588366472139
120;0.28671135574253903;0.10390891991628831
125;0.32373283566975436;0.10429948357021356
130;0.3564747812788602;0.12020173297523203
135;0.3291429847509494;0.10336843693885674
140;0.3423609963692031;0.10819317231127021
145;0.36032684419244315;0.10979028083890369
150;0.26286825070019704;0.10892222923965815
155;0.257822783363359;0.10133948593855528
160;0.4088628402793907;0.11106449358793515
165;0.2747800234970114;0.1011276242035407
170;0.26890288276679786;0.10440620097249281
175;0.24385562008009268;0.1021459139838662
180;0.31071894144937584;0.10681799178835504
185;0.2381234064358173;0.10385750210566619
190;0.22639172933659782;0.09913121541569511
195;0.2950696659956096;0.10542496459761883
200;0.3372870143430501;0.10726533468295239
205;0.32609336032871644;0.1067902184479255
210;0.29071363323478566;0.10370388658265546
215;0.32356220092761107;0.1046003204598371
220;0.32080001531780733;0.1080525376246392
225;0.2614352295846434;0.10762225839255034
230;0.31006257316202446;0.10818537306253304
235;0.31909035823341847;0.10801747889785454
240;0.30594774306044636;0.10196871427855221
245;0.37230807567494334;0.11268768669096856
250;0.2502832789016147;0.09843148058151283
255;0.22526654893510842;0.10302659672957089
260;0.2516049985747622;0.10285963335472584
265;0.2806999854840277;0.1032738639634299
270;0.31334779091429654;0.10652580102900948
275;0.31879668218778356;0.10603983368210558
280;0.36238042473680304;0.11265113515614802
285;0.3293054774520901;0.1081366120974759
290;0.2718501867711063;0.10270714035141866
295;0.23220869505698633;0.101819956453249
300;0.2724851875571693;0.10429942318756955
