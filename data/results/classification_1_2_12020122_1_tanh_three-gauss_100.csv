Train dataset: data/classification/data.three_gauss.train.100.csv
Test dataset: data/classification/data.three_gauss.test.100.csv
Layers: [2, 2, 3]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: softmax
Problem type: classification
Epochs: 300
Epochs;Loss (cross_entropy);Loss (hinge);Accuracy
5;-433.3316924641148;0.1;92.66666666666666
10;-430.6112198763473;0.12;92.66666666666666
15;-426.5565689619323;0.16666666666666666;91.66666666666666
20;-430.6112198763473;0.12;92.66666666666666
25;-427.4331662626661;0.10666666666666667;93.0
30;-428.9893595105813;0.14;92.33333333333333
35;-423.27428347930635;0.09333333333333334;93.0
40;-426.2167709883415;0.12;92.66666666666666
45;-428.2962123800214;0.13333333333333333;92.66666666666666
50;-431.370034124758;0.07333333333333333;93.33333333333333
55;-425.1838258841908;0.08666666666666667;93.0
60;-426.2167709883415;0.12;92.66666666666666
65;-426.2167709883415;0.12;92.66666666666666
70;-428.9893595105813;0.14;92.33333333333333
75;-426.2167709883415;0.12;92.66666666666666
80;-428.2962123800214;0.13333333333333333;92.66666666666666
85;-422.05788820498185;0.10666666666666667;92.66666666666666
90;-426.0333208181678;0.18666666666666668;91.0
95;-426.2167709883415;0.12;92.66666666666666
100;-428.2962123800214;0.13333333333333333;92.66666666666666
105;-428.9893595105813;0.14;92.33333333333333
110;-428.6495615369905;0.09333333333333334;93.33333333333333
115;-428.9893595105813;0.14;92.33333333333333
120;-426.5565689619323;0.16666666666666666;91.66666666666666
125;-426.5701201453107;0.09333333333333334;93.33333333333333
130;-428.2962123800214;0.13333333333333333;92.66666666666666
135;-428.9893595105813;0.14;92.33333333333333
140;-426.5565689619323;0.16666666666666666;91.66666666666666
145;-431.7098320983488;0.12;92.33333333333333
150;-425.34017368760783;0.18;91.33333333333333
155;-427.4331662626661;0.10666666666666667;93.0
160;-428.9893595105813;0.14;92.33333333333333
165;-432.2330802421133;0.1;93.0
170;-427.26326727587065;0.1;93.0
175;-427.4331662626661;0.10666666666666667;93.0
180;-428.9893595105813;0.14;92.33333333333333
185;-426.9099181189015;0.12666666666666668;92.33333333333333
190;-427.4331662626661;0.10666666666666667;93.0
195;-428.9893595105813;0.14;92.33333333333333
200;-428.9893595105813;0.14;92.33333333333333
205;-428.9893595105813;0.14;92.33333333333333
210;-428.9893595105813;0.14;92.33333333333333
215;-424.83047672722165;0.12666666666666668;92.33333333333333
220;-426.2167709883415;0.12;92.66666666666666
225;-428.9893595105813;0.14;92.33333333333333
230;-428.1127622098477;0.2;91.0
235;-427.4331662626661;0.10666666666666667;93.0
240;-426.2167709883415;0.12;92.66666666666666
245;-426.2167709883415;0.12;92.66666666666666
250;-426.5565689619323;0.16666666666666666;91.66666666666666
255;-427.2497160924923;0.17333333333333334;91.33333333333333
260;-428.9893595105813;0.14;92.33333333333333
265;-426.9099181189015;0.12666666666666668;92.33333333333333
270;-425.34017368760783;0.18;91.33333333333333
275;-430.6112198763473;0.12;92.66666666666666
280;-426.2167709883415;0.12;92.66666666666666
285;-426.2167709883415;0.12;92.66666666666666
290;-426.2167709883415;0.12;92.66666666666666
295;-426.2167709883415;0.12;92.66666666666666
300;-428.9893595105813;0.14;92.33333333333333
