Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: indentity
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.3728763811724934;0.07096310776468398
10;0.21936983548308503;0.08966634572495916
15;0.4179520160162629;0.07060491045775076
20;0.30951631395539436;0.07240293947727926
25;0.4274481078003538;0.07056700939616053
30;0.4606404401851677;0.07050491347429787
35;0.36328540764902084;0.07109042440543813
40;0.29419134702911226;0.0730861578884212
45;0.3717221401002925;0.07097725724788732
50;0.3811889568541672;0.07086986136087352
55;0.35646852445058075;0.07119551228235596
60;0.3274663676332771;0.07181827042935894
65;0.2559001171807436;0.07624544910461242
70;0.31313800544163883;0.0722688520881351
75;0.3860345113434047;0.07082204277898116
80;0.27670603585683434;0.07418046014908233
85;0.32899660181715695;0.07177673014107985
90;0.3129572503850621;0.07227533052190761
95;0.2843598987793208;0.07365009793202139
100;0.39869198931898037;0.07071721365142156
105;0.2765178055825382;0.07419470130311986
110;0.22808694399884638;0.08281924316616686
115;0.42874712944810944;0.07056261268127005
120;0.2771750676185258;0.07414524362991572
125;0.3008368687011691;0.07276456988460114
130;0.6364797339215764;0.071086223601956
135;0.4094266851902503;0.07064830483074927
140;0.5181472019414972;0.07057674130471318
145;0.40021673672722935;0.07070637896322235
150;0.5100845808274083;0.07055663331169423
155;0.36653886507466343;0.07104468608929317
160;0.22353862622518195;0.08539396585976494
165;0.3439663893346556;0.0714251221589632
170;0.2648379214896984;0.07521853679932164
175;0.36720578712231006;0.07103564309340117
180;0.23601457465080725;0.08000705782819016
185;0.2259583176765898;0.08388502155696553
190;0.28825997339458803;0.07341232121644291
195;0.45350149730160794;0.07051026516123902
200;0.25059661826525714;0.07699775463915075
205;0.47793650725040326;0.07050691605644002
210;0.23847796312553868;0.07935941400422354
215;0.26344998746058174;0.07536172544568649
220;0.4394382262694844;0.07053302355841938
225;0.21955776646631936;0.08937525300594862
230;0.2536603690773563;0.07654778661413589
235;0.32224140279293484;0.07196908733303425
240;0.3120335360844629;0.0723087813920883
245;0.28481591604711526;0.07362125408363138
250;0.3628176371540477;0.0710972272440159
255;0.2701262083454528;0.07471811078136809
260;0.2367249822018531;0.07981221982180856
265;0.452413877589359;0.07051142882385049
270;0.22939549710969748;0.08224926715542086
275;0.28049750301667525;0.07390634147061752
280;0.30236412172420024;0.07269645870178244
285;0.3952452964129608;0.07074306640937655
290;0.3188965395093577;0.07207343964344416
295;0.26385341707558074;0.07531955456239396
300;0.2316522578653753;0.08138162915110496
