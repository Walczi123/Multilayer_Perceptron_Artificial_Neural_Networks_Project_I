Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.3680302980993857;0.13757487427097306
10;0.23603510966302962;0.11590852702889187
15;0.3928917096668747;0.13010872152272085
20;0.2793228505469308;0.11416963085643825
25;0.3986072072487418;0.13249594249077484
30;0.31390747749639153;0.11324979270990283
35;0.4078760045137453;0.1327667390068807
40;0.2646901136175073;0.10522904500669715
45;0.26537689412356735;0.0981102578608506
50;0.2759712755250367;0.12872947186434747
55;0.30218129671553645;0.12142481301446337
60;0.30820959637835993;0.11423265822258538
65;0.31683559371907155;0.1156937853548781
70;0.2811632127172288;0.11020339032355887
75;0.2862626588783668;0.10944788696280472
80;0.2755509710381208;0.10845733630419019
85;0.3660472562291382;0.12812126206767063
90;0.28153612983201365;0.112443575792534
95;0.24483547956642007;0.10715180822286202
100;0.3191754784126703;0.12477306973238117
105;0.23945191854147002;0.11560138146184855
110;0.28403832458777123;0.11430797443300383
115;0.35281785092247536;0.12355314008240013
120;0.2695584432677394;0.11201929136719917
125;0.29326380228675525;0.12613740607984666
130;0.35224457032734385;0.12483673577467141
135;0.2999167012333499;0.11554858502788053
140;0.3240989765984698;0.11547620549725364
145;0.39138309564669505;0.1296914759950491
150;0.32331281432798614;0.12029137984372136
155;0.2917202832233022;0.10940527644178184
160;0.2696798452849337;0.11533001576201718
165;0.2917818761866933;0.10855102882896529
170;0.275431486825768;0.1077812820841548
175;0.306693645463545;0.11123698654280582
180;0.2618107213591326;0.10710182997309073
185;0.2593980462669793;0.1016598660670573
190;0.2762715348477599;0.10953805907250228
195;0.2998705530715338;0.10878704211157192
200;0.36898095230313455;0.12270977768634568
205;0.40537167851496164;0.13198181088233898
210;0.24402924868551648;0.10167121827538654
215;0.31113839298284135;0.11889813855952255
220;0.3158735537203382;0.11479187892794343
225;0.2458646762067746;0.09884131253117089
230;0.3398009356623175;0.11420762538800071
235;0.3507010457499556;0.115726562722423
240;0.3464544985895873;0.1265654015632799
245;0.25477533581624134;0.10542889872053447
250;0.374524240346648;0.1280737600753415
255;0.2629796131338332;0.11092366560833095
260;0.2593475395166823;0.10009298417422915
265;0.3121890317629397;0.10504529964053162
270;0.24334149118518053;0.10557379993339963
275;0.33321915833604077;0.12005481452878963
280;0.2950105652140349;0.11610761517366285
285;0.2945135451765403;0.1099058497196444
290;0.2644824771753402;0.1060366375057428
295;0.2509855778367741;0.10241920619638978
300;0.24977068179736345;0.1057583543240868
