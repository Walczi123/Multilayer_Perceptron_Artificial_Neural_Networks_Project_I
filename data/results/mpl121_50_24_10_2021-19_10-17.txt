Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 2, 1] with biases
Seed: 12020122
Epochs	Loss
2	[0.01360427 0.01350199 0.0134004  ... 0.00031264 0.00032092 0.00032934]
4	[0.01030612 0.01021712 0.01012877 ... 0.00107584 0.00109115 0.00110664]
6	[0.00971855 0.00963213 0.00954635 ... 0.00127709 0.00129377 0.00131062]
8	[0.00958757 0.00950174 0.00941655 ... 0.00132517 0.00134216 0.00135933]
10	[0.00955919 0.00947348 0.00938841 ... 0.00133576 0.00135281 0.00137004]
12	[0.00955344 0.00946776 0.00938272 ... 0.00133791 0.00135497 0.00137222]
14	[0.00955234 0.00946667 0.00938163 ... 0.00133832 0.00135539 0.00137264]
16	[0.00955214 0.00946647 0.00938144 ... 0.00133839 0.00135546 0.00137271]
18	[0.00955211 0.00946644 0.0093814  ... 0.0013384  0.00135547 0.00137272]
20	[0.0095521  0.00946643 0.0093814  ... 0.0013384  0.00135548 0.00137273]
22	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
24	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
26	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
28	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
30	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
32	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
34	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
36	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
38	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
40	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
42	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
44	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
46	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
48	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
50	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
52	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
54	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
56	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
58	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
60	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
62	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
64	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
66	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
68	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
70	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
72	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
74	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
76	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
78	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
80	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
82	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
84	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
86	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
88	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
90	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
92	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
94	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
96	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
98	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
100	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
