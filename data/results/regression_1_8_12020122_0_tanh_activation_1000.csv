Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.37314181140992936;0.1035491103289644
10;0.2683852855975793;0.10504958159792344
15;0.32704895021781183;0.10268729068881854
20;0.24263322269012969;0.11741478460254645
25;0.4047876758985202;0.09988496180036521
30;0.294492488477799;0.09695824340655507
35;0.28510623335322627;0.10275145682880225
40;0.2580125380701526;0.10187621142479725
45;0.3832976247476698;0.10799865561621032
50;0.33020913983832156;0.10536510380667566
55;0.35446107248440495;0.10273901231888471
60;0.27412245323265644;0.10275493550625937
65;0.29509208932746467;0.10342497926120818
70;0.3462824901984775;0.10068762738745589
75;0.3352080159041001;0.10523611208534166
80;0.2932833883652182;0.10374151805336236
85;0.2918126541454045;0.09566001334756334
90;0.2840805730004179;0.10224119766435696
95;0.3394055994935346;0.10792346183002842
100;0.34851601733605314;0.09778430568327214
105;0.5445259357153562;0.11042691495224695
110;0.2908727674298689;0.1005891342590933
115;0.2724058239684224;0.10304565923570802
120;0.33838932983659414;0.0997903700370592
125;0.32588103183059514;0.0989777401510138
130;0.41239365013985124;0.12436922636033848
135;0.3656304138350134;0.09789079705986181
140;0.38798786492675386;0.10875409118613649
145;0.43157705390575896;0.10757902710440093
150;0.2672001328976679;0.11262974856633506
155;0.25278779444509686;0.09718519689132493
160;0.533464573571256;0.1082884563145079
165;0.3222026964743237;0.09666565527739465
170;0.23737940110660777;0.1105574774252445
175;0.25652232838270006;0.09856383873548641
180;0.42057681890251125;0.10832593133336264
185;0.2592883954640653;0.10479872866633554
190;0.24190180977069653;0.09513165187382101
195;0.3110056362334348;0.10374833936815692
200;0.38189774835374546;0.10435230438653893
205;0.35794605068132845;0.10363828459411491
210;0.3454210822735452;0.10410930153835601
215;0.34098587037178496;0.09808012662032754
220;0.3139117951576987;0.10722518495788741
225;0.2530985662708416;0.11156150725525443
230;0.28537560066490236;0.10693947620838554
235;0.36493824692979143;0.10648477013088192
240;0.31561917094759806;0.09730539274683388
245;0.452081557855971;0.11135421854084747
250;0.24066601492375025;0.09921238594245245
255;0.2328366181115461;0.10226235942406554
260;0.2328053128500179;0.11295183274154783
265;0.3374044138019408;0.10449209767288103
270;0.31318823429425013;0.10278889728912866
275;0.355388561080794;0.10141418504104463
280;0.41903519362503866;0.11109264752216856
285;0.33134844606732694;0.10918308109929156
290;0.3024910443551162;0.09948245002971284
295;0.2738131341528604;0.09602430510392258
300;0.2725843673911883;0.10330231714741386
