Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 8, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.4026590397105239;0.2916523643363863
10;0.3694128689663344;0.24618983784362325
15;0.41102472334542517;0.23100221965774503
20;0.36711858090816535;0.32547227950934304
25;0.44414972250066725;0.25014941605982427
30;0.38544046764137785;0.27327054230339126
35;0.44159483220427226;0.23247240556840396
40;0.46649946101728934;0.24954880276184455
45;0.42505978061372335;0.21918212620670413
50;0.3729871422076102;0.3078748314137385
55;0.40485063158763207;0.26024524952158073
60;0.40634696808643717;0.2748012864306057
65;0.47854609582115887;0.22524916462499556
70;0.4289409771796421;0.2408043780861685
75;0.3924696247699742;0.2266698678732791
80;0.48049090383670534;0.2292627969771315
85;0.46722988961461803;0.20602713030880113
90;0.3875036977263965;0.33538918301482806
95;0.4972051876494263;0.22871155006457292
100;0.45415851925904965;0.24685916391729607
105;0.5362474654750793;0.23020769968565896
110;0.41587310730694466;0.2566598573747372
115;0.39657025210688673;0.26006609338276865
120;0.4414546420447959;0.24210667199235592
125;0.3733834669302334;0.3254609293986471
130;0.4148702198585621;0.25270570025340355
135;0.43651928495279324;0.23005751452501486
140;0.3780764648410865;0.3377973529270711
145;0.38565707651444875;0.24242863661533456
150;0.4222206446005996;0.25631096293906014
155;0.406148526600911;0.31016242276756256
160;0.4702705843964855;0.24497576953775108
165;0.3664365409736027;0.2709994082191148
170;0.45799370129964084;0.24008662687740465
175;0.3990971104320398;0.2560548332469475
180;0.4773489688291677;0.22865289059137445
185;0.5047366348697512;0.24559987300734765
190;0.4128814671632723;0.23722065956868205
195;0.39580436203049774;0.24438817086482584
200;0.4289624323530249;0.24672274282587
205;0.4327727315557644;0.20115650463087045
210;0.4847141110449781;0.2565393387141375
215;0.40847146458835626;0.280492280796023
220;0.5371073197207411;0.23969189356866033
225;0.3918390823423149;0.2470681385582497
230;0.42835916707030663;0.25010355448947674
235;0.37323346899268117;0.2678407620762138
240;0.37076973753630604;0.32544729724661703
245;0.4024533396815054;0.2160091795212781
250;0.4735516055217676;0.2269672080215444
255;0.40067007353652495;0.25541979314184815
260;0.43963312414488914;0.2440766146165123
265;0.3958279340556665;0.25996707417650583
270;0.41223297686388266;0.22845881903787146
275;0.3870123799636961;0.21924606437750677
280;0.4111764959117677;0.26816373494306855
285;0.40728037680824375;0.23692844208732083
290;0.45925207147530267;0.23938048096703252
295;0.4013561319269823;0.24182139927239873
300;0.4951350350803641;0.23140880848949427
