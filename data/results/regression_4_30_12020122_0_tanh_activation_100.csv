Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 16, 8, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.3847648151126187;0.17052175219468169
10;0.2375396464841827;0.10338887124341087
15;0.34768505218363227;0.14062869554849672
20;0.30447078359513885;0.1243972889414999
25;0.3634427561059398;0.14834366831377344
30;0.3018996488822278;0.12260704483191283
35;0.3634007149684679;0.14211514975008735
40;0.282298309060974;0.1284983769759079
45;0.3187590733125586;0.1132653121556393
50;0.28608353291411953;0.14044528584446456
55;0.32321157360516123;0.13189326617453062
60;0.3511282904502054;0.13958891686496486
65;0.32254712835518123;0.12962824206317766
70;0.29383080018603475;0.11942674192592563
75;0.3004004921839121;0.11858856807868245
80;0.3007910912338595;0.11236838692483019
85;0.37172048595531404;0.1500592944790832
90;0.3274981973991762;0.1349872645453929
95;0.28733202092569127;0.11447578189741933
100;0.3193368064899458;0.1363481658749205
105;0.2681524934401109;0.12182608876146357
110;0.3045036751515408;0.12652736521028937
115;0.3850493166969381;0.1380067688257194
120;0.27516619823945493;0.1184648520850965
125;0.28945997700713827;0.13582950989684714
130;0.35199154885201683;0.13153295190594047
135;0.28553263572962634;0.13340931451526442
140;0.32932564363755173;0.14857808423052707
145;0.40783516838421646;0.16036128819180792
150;0.32880473542529526;0.14350556442281195
155;0.28907019238269266;0.11811960150282691
160;0.3157800398231119;0.13936523221142477
165;0.33580298281409493;0.13292832117218023
170;0.3178262410892385;0.13527756450342612
175;0.3151024192841346;0.1266797170125039
180;0.2877352919610282;0.1225813036418669
185;0.32118749222109166;0.12634746500569943
190;0.3139453644702498;0.13088222442183625
195;0.3315746012384787;0.13627423791381868
200;0.31965934022825876;0.1323160219691304
205;0.38773916931082775;0.14927197415414564
210;0.3030715749359133;0.12316954974610052
215;0.3232958034600898;0.1355909253112016
220;0.31883288036691354;0.1364918447312292
225;0.30660692376647936;0.1278676194843506
230;0.34093948337721525;0.13724730191414575
235;0.3386241419104;0.13380464323273739
240;0.3489999836991709;0.14823735389497295
245;0.30401423584393744;0.12661311394538588
250;0.3503030594683687;0.14570621239630996
255;0.27557769710983887;0.12735841818403512
260;0.3058771094714582;0.1321812564346699
265;0.3370720929234033;0.1364657333626407
270;0.2691209695290274;0.12688992458194467
275;0.33734421068234177;0.13506949660564446
280;0.3260072458782259;0.13595531381864942
285;0.3055956336414631;0.13287948971123795
290;0.2994927905669315;0.12211811128759029
295;0.2947144063736652;0.13185039425544606
300;0.3020079817667192;0.13118470682985817
