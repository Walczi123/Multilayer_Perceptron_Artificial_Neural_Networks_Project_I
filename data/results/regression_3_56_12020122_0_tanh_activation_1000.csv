Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 32, 16, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.32123364942973637;0.11484481237357015
10;0.29964627482320705;0.12615140517617657
15;0.31662243255538286;0.13616282612680625
20;0.2680947783789389;0.12350584488185277
25;0.4149134758851767;0.14111409238901204
30;0.2988643130920602;0.12631785859066438
35;0.31291532548903217;0.1242397854598279
40;0.3064408457380305;0.13428341342461458
45;0.33305057040597347;0.12870324058165808
50;0.33889466003969365;0.14242687381984465
55;0.34147516126161515;0.12871828564649118
60;0.30238045284438164;0.13603435593582455
65;0.3229868473213723;0.13205145269974417
70;0.3118060460813674;0.13649292749364458
75;0.33432872063881164;0.13298023013520707
80;0.2975819334750825;0.13100621616734887
85;0.31284989260236623;0.12925201368968142
90;0.29905364149210734;0.14612546204049576
95;0.37467934606506204;0.14910596660735356
100;0.3364640727175875;0.12920738591969683
105;0.3843541763123858;0.12947308800955648
110;0.322088478104649;0.13517563288154272
115;0.3043219866083219;0.1309380244120818
120;0.3431274512963295;0.13453869265101548
125;0.34569988447098715;0.13589842542824038
130;0.422323881707229;0.1395517235553113
135;0.36180679623459255;0.1336008889924726
140;0.3507342520491925;0.13055471616147926
145;0.4121137182796738;0.13445362056442922
150;0.27392242637088743;0.13959379240788872
155;0.29383851661397475;0.12660130365146396
160;0.39301360222648335;0.13424731827174896
165;0.3184561571823706;0.1281660717295919
170;0.2456570504447575;0.18048778132098098
175;0.29692239660635705;0.13010596428404947
180;0.32506210773105954;0.13394305297907896
185;0.2788860410187334;0.1433950298302081
190;0.2980996864119076;0.1331629870999729
195;0.315370570306827;0.13616626022796893
200;0.3419162641608914;0.13641479388002722
205;0.32947353971690696;0.13707109794979358
210;0.31346206101870555;0.13691905895726356
215;0.31215759287284817;0.13521089963383753
220;0.3384867622485989;0.14305433094860118
225;0.2702877066064169;0.15274998837951215
230;0.2882956912633359;0.14225501589913453
235;0.3282540400719265;0.138011407590804
240;0.33054703296245685;0.13167950991060068
245;0.35021497718753525;0.1435732277785355
250;0.3193788262550292;0.1375502257613276
255;0.2883088374622768;0.13076748851908657
260;0.27057779377187124;0.13912976888023862
265;0.31815933236375027;0.13851675506316427
270;0.3402064478223035;0.13923331456946464
275;0.34107820843391773;0.1322911708849869
280;0.39729234422903315;0.1413744693231849
285;0.324281386276701;0.13861763852275938
290;0.28642125429132037;0.1404896300918026
295;0.3341747811864066;0.1325654774767205
300;0.3074013942255253;0.13818043884809067
