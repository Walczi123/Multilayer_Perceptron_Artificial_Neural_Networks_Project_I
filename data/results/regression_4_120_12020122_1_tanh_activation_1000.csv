Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 64, 32, 16, 8, 1]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.0006591999368625927;0.03144801585498058
10;0.000790818585144571;0.03250781211424734
15;0.000834984326937479;0.03298074499463498
20;0.0007307300422126307;0.03179304283669973
25;0.0007513469893312274;0.03214432822124042
30;0.0006604272113077974;0.03106515355808445
35;0.000620585130331544;0.03076061677800736
40;0.0006408196030042368;0.031230867438247482
45;0.0006751363596494563;0.031858272076326896
50;0.0006144561461797326;0.031290030501862874
55;0.0006221019515991419;0.03147371551647598
60;0.0005736954141644023;0.03097316308705138
65;0.0005638918831066163;0.030998892239213358
70;0.0005273453812610233;0.03058959048104938
75;0.0004749228629162818;0.029819876437797585
80;0.0005117326777805743;0.03051940378312597
85;0.0004829029111248866;0.030216291504234483
90;0.0004509563893294747;0.029839009389211243
95;0.00044275739705767705;0.029718237011914225
100;0.00045064585205500864;0.029988567989733098
105;0.0004170477804276731;0.029465100010050087
110;0.0004351943124569685;0.029911127863793442
115;0.00041009705026507596;0.029567479416424437
120;0.00040258757130550907;0.02947136814022358
125;0.00036934279029533535;0.028854242799904513
130;0.0003581705990439989;0.028686515480183476
135;0.0003644976068395725;0.028925496763615936
140;0.00036647032656876913;0.028996756857230816
145;0.00034658152568213285;0.028631584317247757
150;0.00035663847290399314;0.02892200854735062
155;0.0003078711325584503;0.02780894496646693
160;0.00032797463061570107;0.028389136506255803
165;0.0003341037903782104;0.028630584207841012
170;0.00031235646935934524;0.02810095270116524
175;0.0003283646395015354;0.028593544277310593
180;0.00029567051491258424;0.027801759186235956
185;0.00031450202162561566;0.028310772755574122
190;0.000296535342155916;0.027928444052658673
195;0.00029355774035410946;0.027895093802935193
200;0.0002894367114554676;0.027831917166035842
205;0.00029388042292321137;0.02797440026048278
210;0.0002729801969720376;0.027473866706356348
215;0.0002787562103116617;0.02766305464058432
220;0.00028622866151448227;0.02791251437778145
225;0.0002806973754831322;0.02776746222531998
230;0.00027506818135638163;0.027647568468401645
235;0.00027919543196032416;0.027815853943715756
240;0.00027459682966368687;0.027722665941731207
245;0.00027147121694179435;0.027669460506716707
250;0.00026675809919062595;0.02754642668080289
255;0.0002452202625842249;0.02690690929118819
260;0.0002504371108782172;0.027117920586311926
265;0.0002596734833170464;0.027433095086063005
270;0.00026114977184617087;0.027493590026425888
275;0.00026252590699660846;0.02756640255207821
280;0.0002476234889438832;0.027148775585276466
285;0.00025607017608599323;0.02742083190250013
290;0.00024623101090926304;0.027134830426168183
295;0.0002492167073789861;0.027257807472098047
300;0.0002568025012646787;0.02750629713220425
