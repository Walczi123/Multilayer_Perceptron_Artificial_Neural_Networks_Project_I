Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 32, 16, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.35159480528669657;0.13768893414990166
10;0.23902143243783897;0.15490829385461202
15;0.3831052560931345;0.12403538766064723
20;0.26399795471584325;0.1172816938284162
25;0.3639075454673279;0.13206985771261884
30;0.32013271028265583;0.11120450987517097
35;0.37878787364175664;0.13684701380232378
40;0.29014069251474445;0.11917762579257095
45;0.3204619324418027;0.09923876508900302
50;0.2666442869386625;0.1411342426880337
55;0.3131896516704796;0.12445563160488493
60;0.35055153988234533;0.1255415195445647
65;0.3108673750770886;0.12027665003911021
70;0.30377818428700915;0.11572642801840191
75;0.30423925368993765;0.11758791633893954
80;0.303613661507287;0.11305676531480531
85;0.3665043516432346;0.13833617407226925
90;0.31519112225561857;0.1302467312172584
95;0.2695270242097357;0.120042003075183
100;0.33132127672059186;0.13153339330191616
105;0.2733318640393492;0.12198317810485754
110;0.30529764072021065;0.12375353263929573
115;0.42119090721091995;0.13105121154077726
120;0.28197165053215445;0.11666382188861327
125;0.31685065688991165;0.1272171045244932
130;0.36809361066064855;0.12319124998325705
135;0.29580740413361706;0.13371241591841726
140;0.33367130425595676;0.13469973344466477
145;0.4065938062780756;0.1432057684976586
150;0.34077650994968833;0.12985423754921646
155;0.2995334164396037;0.12275249785460789
160;0.28947358568995984;0.1363775193419525
165;0.357615262628638;0.1279881192527955
170;0.3222333430347565;0.1296318535127428
175;0.3198451957465014;0.1287742187722185
180;0.28666514786994957;0.12663814423412867
185;0.3084468476525427;0.11966509657795471
190;0.3015224193856985;0.1278097074113454
195;0.3191466718331004;0.13400085335638653
200;0.3372026295430425;0.1295613142850622
205;0.3751172294333357;0.13771163311434662
210;0.2812637097579001;0.12333296287852831
215;0.3319742210704504;0.13593316374523795
220;0.3225173976259726;0.13175858196099696
225;0.2881814675866808;0.12798825971803318
230;0.3359330760492495;0.12618682519315133
235;0.38677307248426923;0.1258528358660264
240;0.3408399615102749;0.14053802187872116
245;0.30525481430697293;0.1241952866711289
250;0.34624402440847596;0.13561472784147377
255;0.27037421747175283;0.12738566849343985
260;0.2989236992840681;0.13367837297663976
265;0.33875384099924694;0.1325075877986692
270;0.2534859145316184;0.1338646221606131
275;0.32995137116190554;0.12742510861235626
280;0.3377048696778233;0.12863930137256743
285;0.3145285973197943;0.1300966223894985
290;0.30125360223901776;0.12246369285845299
295;0.29058376241801015;0.13237385211458813
300;0.2632101353615061;0.1384939863663564
