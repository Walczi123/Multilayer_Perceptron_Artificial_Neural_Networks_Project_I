Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 64, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.3201896472801213;0.11011216507184465
10;0.29971632927420566;0.10196452639341354
15;0.32019962979318695;0.11855256774098848
20;0.2416944941713738;0.1211483689217878
25;0.39576806509439455;0.11967215911522856
30;0.3556911734473526;0.1051010591822548
35;0.2893067988364025;0.1060817546067425
40;0.30769183296817815;0.09650656496748383
45;0.33613019131663446;0.1174490396083972
50;0.32774589848867697;0.13380343521023086
55;0.29665604346924807;0.11142464904432801
60;0.30691179053723433;0.111472068307804
65;0.32968134166938456;0.10903377511258429
70;0.3777513685337507;0.11088217081327544
75;0.39908146784905824;0.10983310952904832
80;0.34729207427263314;0.10225567658456108
85;0.3271143923083225;0.09562542620179369
90;0.3380194366265099;0.11331314066637224
95;0.33939834014587666;0.14012407268289398
100;0.40947475071722694;0.10957846241886769
105;0.43537704243550873;0.12289777917082569
110;0.32133369374474235;0.12184818019386083
115;0.3359521117281929;0.12887007427992353
120;0.34945056928493456;0.1048297454221887
125;0.2805447752326627;0.11992833517109534
130;0.36692955424348156;0.13924059625719795
135;0.31255019381605387;0.12146883345897909
140;0.3500626246557793;0.12556419673746397
145;0.3981106898719259;0.12727362089457836
150;0.2913584318273995;0.12496895029610246
155;0.3030888785924654;0.11019331890413725
160;0.4199568731040343;0.12693841092632385
165;0.37737212283676885;0.10416113171228931
170;0.27995272053233056;0.28159405814284055
175;0.29563477252898385;0.10523229282763966
180;0.3687147019048349;0.1216814412125225
185;0.27757178849708886;0.11343835597566586
190;0.29654173383541427;0.10745226487637485
195;0.3074015143842237;0.11436807265593214
200;0.325355013139386;0.11186959102950618
205;0.33282291656123036;0.11382277647764148
210;0.40070897782424686;0.12092061401037052
215;0.36258364718111014;0.10953106860190935
220;0.30815837182405775;0.11775078047947168
225;0.30577742889351023;0.11352100886137667
230;0.3242518620686868;0.13231382077805554
235;0.36470539351136394;0.10998674899297364
240;0.3052163692920351;0.11914920469223324
245;0.41547493267533514;0.12467158585505116
250;0.24883149452938988;0.12587302856522373
255;0.2473449471005707;0.1174086716174486
260;0.23926418256993442;0.18275679618169433
265;0.33475673145497903;0.11695486800168857
270;0.3261238168818312;0.12124825581557429
275;0.32743570707871994;0.12229679052905704
280;0.39465820008708896;0.13372774814636565
285;0.30868286649307214;0.1303981048011462
290;0.2805481127310757;0.1163032517737419
295;0.24455742464712452;0.20064439685950952
300;0.4179611022258805;0.10472248945561694
