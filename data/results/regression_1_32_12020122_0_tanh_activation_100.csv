Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.44445967741408693;0.10240704392649325
10;0.3536797471734257;0.3118261511887882
15;0.36155043316961455;0.09104802015934818
20;0.3074819485627846;0.2263598626875382
25;0.6012010659825269;0.11244597475853838
30;0.5735839871328845;0.10163552416380478
35;0.554448447246786;0.10882199612967114
40;0.23149476041294273;0.10614147627550946
45;0.28083951456845496;0.08180137618576029
50;0.6015294588081296;0.11142198483662552
55;0.5086413128963264;0.10199539942786362
60;0.25365871889507324;0.09585700750276083
65;0.27353123842090843;0.08830249074214333
70;0.24237349161524954;0.10766156714772414
75;0.24439225597297023;0.09510840998343967
80;0.28436773314959074;0.09651415681441727
85;0.47343124891961963;0.10037977151285452
90;0.3031533464120571;0.09445354807123381
95;0.2574974668288289;0.13201338042126148
100;0.5152477643055046;0.10674394866153558
105;0.3102674026096369;0.09912243397578316
110;0.34999704248106156;0.10742146700486814
115;0.7922505043638552;0.1168096966612369
120;0.43555206206624775;0.10431133553355267
125;0.31577941038439755;0.10625220323112837
130;0.44403130056529283;0.11634202449402288
135;0.37523010201010276;0.1061515766707548
140;0.3581285322062681;0.0993310303779449
145;0.6561621255663045;0.11605820646322912
150;0.46169640539998236;0.10215384641926505
155;0.33615739984150644;0.0982685246683959
160;0.27394502327455467;0.1702472849371247
165;0.26137822066534644;0.09664483525610115
170;0.23887725517877823;0.09708855578777888
175;0.3214660509730781;0.09097664084671851
180;0.23579283022204026;0.1095228926629789
185;0.2460375085738696;0.11018899502865968
190;0.26734697989974116;0.09972907537384497
195;0.3566699461185514;0.09880339964766298
200;0.3163410163528068;0.09514251767873387
205;0.5583920701609796;0.10891528587763315
210;0.247357753376642;0.09992601378812693
215;0.2967057226335747;0.11004334778665376
220;0.3433877230585503;0.10339483686667172
225;0.267144662444902;0.15643322565147777
230;0.3048866890306823;0.09761485890318933
235;0.4984112182997541;0.1008677483010447
240;0.36510400085178746;0.10552738755982652
245;0.237546847256653;0.1025126822247771
250;0.3752621846252996;0.09915334724633544
255;0.2720393345494081;0.09999586178039035
260;0.24383375942469676;0.10071769697154034
265;0.37454990240671926;0.09529742021819683
270;0.2572877157835886;0.1020895890501038
275;0.3012510471710369;0.1043145089474717
280;0.3282200732415408;0.10140733434353295
285;0.3540507178533908;0.10070400977223606
290;0.29288973778666366;0.09487561745074775
295;0.23609496391397328;0.10683363861106951
300;0.26691289355789627;0.15142110964096686
