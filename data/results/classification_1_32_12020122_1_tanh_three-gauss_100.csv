Train dataset: data/classification/data.three_gauss.train.100.csv
Test dataset: data/classification/data.three_gauss.test.100.csv
Layers: [2, 32, 3]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: softmax
Problem type: classification
Epochs: 300
Epochs;Loss (cross_entropy);Loss (hinge);Accuracy
5;-441.6630092142125;0.06666666666666667;93.0
10;-433.8549406078793;0.08;93.33333333333333
15;-434.9535528298808;0.08;93.0
20;-436.5754131956468;0.06;93.33333333333333
25;-441.8464593843862;0.06666666666666667;93.0
30;-437.3206762606791;0.10666666666666667;92.66666666666666
35;-432.53431346461014;0.03333333333333333;94.0
40;-429.8138408768427;0.05333333333333334;94.0
45;-430.03585579811045;0.1;93.66666666666667
50;-460.35296817161543;0.06666666666666667;93.33333333333333
55;-450.6353571603978;0.06666666666666667;93.0
60;-434.09050671252544;0.05333333333333334;93.33333333333333
65;-431.370034124758;0.07333333333333333;93.33333333333333
70;-460.81054919752927;0.05333333333333334;92.66666666666666
75;-437.3863433785298;0.05333333333333334;93.66666666666667
80;-449.70664392519177;0.04666666666666667;93.33333333333333
85;-453.8790778919298;0.05333333333333334;93.33333333333333
90;-458.37775864888033;0.06;92.66666666666666
95;-442.5917224494185;0.06666666666666667;93.33333333333333
100;-441.7807922665355;0.05333333333333334;93.66666666666667
105;-446.29302420686435;0.06;93.66666666666667
110;-458.6133247535264;0.05333333333333334;93.33333333333333
115;-458.73110780584943;0.04;94.0
120;-449.4189618860733;0.06;93.33333333333333
125;-448.72581475551334;0.05333333333333334;93.66666666666667
130;-442.64383838389085;0.08;93.33333333333333
135;-450.92303919951627;0.05333333333333334;93.0
140;-440.9041949658018;0.12;92.66666666666666
145;-462.5501926156183;0.06666666666666667;92.66666666666666
150;-454.1010928131975;0.04666666666666667;93.33333333333333
155;-459.7119369755278;0.05333333333333334;93.0
160;-458.20785966208484;0.06;93.33333333333333
165;-464.10638586353366;0.05333333333333334;93.0
170;-460.64065021073384;0.05333333333333334;93.33333333333333
175;-460.64065021073384;0.05333333333333334;93.33333333333333
180;-458.20785966208484;0.06;93.33333333333333
185;-458.6133247535264;0.05333333333333334;93.33333333333333
190;-450.92303919951627;0.05333333333333334;93.0
195;-459.7119369755278;0.05333333333333334;93.0
200;-458.20785966208484;0.06;93.33333333333333
205;-454.9120229960805;0.06;93.0
210;-462.6023085500907;0.06;93.33333333333333
215;-445.429978089509;0.05333333333333334;93.33333333333333
220;-439.53145188806025;0.06;93.66666666666667
225;-453.12026364351914;0.05333333333333334;93.66666666666667
230;-454.1010928131975;0.04666666666666667;93.33333333333333
235;-439.93691697950175;0.05333333333333334;93.66666666666667
240;-448.32034966407184;0.06;93.66666666666667
245;-441.4931102274171;0.06666666666666667;93.66666666666667
250;-463.0077736415322;0.05333333333333334;93.33333333333333
255;-454.96413893055296;0.07333333333333333;93.0
260;-450.51757410807477;0.06;93.0
265;-448.6080317031903;0.04666666666666667;93.66666666666667
270;-456.1805342048774;0.06;93.33333333333333
275;-463.0077736415322;0.05333333333333334;93.33333333333333
280;-439.93691697950175;0.05333333333333334;93.66666666666667
285;-455.31748808752207;0.05333333333333334;93.0
290;-452.88469753887307;0.06;93.0
295;-445.0245129980675;0.06;93.33333333333333
300;-459.7119369755278;0.05333333333333334;93.0
