Train dataset: data/regression/data.cube.train.1000.csv
Test dataset: data/regression/data.cube.test.1000.csv
Layers: [1, 16, 8, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.4397640020179495;0.23328600142078237
10;0.4546779522585624;0.2377103247896023
15;0.4332169475578644;0.2663134567436903
20;0.4597799004959125;0.25653824471495545
25;0.48339895981662234;0.24788000839414967
30;0.43833401690684565;0.3051917496274005
35;0.3895044980135949;0.6329149346375094
40;0.49025677145158575;0.2737506665250962
45;0.5433624324562223;0.2758468309051062
50;0.5005035830165239;0.25120191846046064
55;0.47598435085250346;0.2329351784135219
60;0.4828816302953753;0.24799508116159363
65;0.44657516040696604;0.2611730099625311
70;0.4488515222759561;0.2589950518298785
75;0.4533172911291769;0.25353162855441086
80;0.4859743548091062;0.28108156264429335
85;0.4377185093707119;0.25650050392898627
90;0.41164968925855355;0.2651579391494022
95;0.4437598528891726;0.26932660347012033
100;0.48219409467382013;0.27662293194952714
105;0.4642567009078654;0.283004309563722
110;0.40861918580844087;0.322892965432256
115;0.4457683903311187;0.255851272265842
120;0.4743548294387766;0.2784600419533247
125;0.43884450937420244;0.2677839942142404
130;0.5093742334741451;0.28172133545414385
135;0.4600133557200914;0.2429245914299487
140;0.5654875158434726;0.264263994682179
145;0.4714054730326526;0.26482685665265865
150;0.48978514317267474;0.27479993807812575
155;0.4856507055827621;0.2672500516791115
160;0.40814479278647264;0.3163140618614212
165;0.4359233582197816;0.2419514294199668
170;0.38260972549174704;0.3302477011641932
175;0.4258787799598084;0.3273697594723399
180;0.4857749781407972;0.24937837462438253
185;0.5025523786325428;0.27085975262887774
190;0.4201412277005907;0.27594479162640734
195;0.49236321327973837;0.3050653743218168
200;0.4500961806114069;0.30903279088347835
205;0.4526161026181506;0.28560435589832334
210;0.4751430805914336;0.2674842734224922
215;0.45321059274371156;0.2607302202246389
220;0.4638867174592864;0.2640328988622638
225;0.5758226250927347;0.2651980568219216
230;0.4411152533934576;0.2634800498727228
235;0.5028430613019255;0.23858938360372145
240;0.6141457680190969;0.2552757833687588
245;0.44106668247748554;0.2497468932242444
250;0.47976633455472373;0.2541008176893463
255;0.456987356274651;0.27339776345905376
260;0.4564273187112813;0.2791990770032788
265;0.47503293048702433;0.25328782892619883
270;0.4830462452543675;0.25020924497315256
275;0.4542972090076831;0.2532907819016468
280;0.4216117827977229;0.2841115728057639
285;0.4409301886971227;0.2644294350734014
290;0.4839365107390079;0.28115483909850064
295;0.4554381708192956;0.27098805280913446
300;0.4677949402213283;0.26361635385642285
