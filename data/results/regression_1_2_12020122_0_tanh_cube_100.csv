Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.40210733580217684;0.2165023625605822
10;0.36027455168022127;0.2552643279152193
15;0.4014399108854548;0.2037685442387201
20;0.35513659679389953;0.3177838413528925
25;0.43174349941240014;0.19664536096283028
30;0.3777978044220274;0.22239071063274868
35;0.44184100629209266;0.19363277244196847
40;0.4558024600598529;0.1953004846346192
45;0.4353196708630604;0.19577328082221565
50;0.3716656575854847;0.24427748057579493
55;0.42031207928790937;0.20198232466538676
60;0.4097489618240736;0.2081074502888495
65;0.4668249397595378;0.19466317830887
70;0.41366715632543194;0.2063664983083862
75;0.39691565155399783;0.2119005727797286
80;0.45811115580022355;0.19406466427329508
85;0.40495251370415986;0.2074445717717559
90;0.3728224544478874;0.23765544524892418
95;0.46522848080136464;0.19387083537330985
100;0.4625401899254271;0.1986608142185766
105;0.5091655992160365;0.18876927804420732
110;0.428140336654885;0.19831595251536313
115;0.37520440936889454;0.2350940358107082
120;0.4493570657060655;0.19241505819784696
125;0.35785576543956965;0.2733910901571913
130;0.4158369282196224;0.20206979927376303
135;0.4433520276963391;0.19518446751522872
140;0.36689757929944733;0.26589570901934423
145;0.39214566846375853;0.21057935779704218
150;0.4097177884969359;0.2065531398799263
155;0.40231077736964593;0.21468458414610422
160;0.4444322125811242;0.19950007194139255
165;0.3538179414631866;0.349339773357926
170;0.45073052227615396;0.20033910811754685
175;0.4054963981415851;0.2125359123871192
180;0.4528329462724459;0.19511133146379947
185;0.48316029043116454;0.19417497733470498
190;0.4122120358434107;0.2026892852252939
195;0.38829652563583306;0.21115526150062525
200;0.42463983061366933;0.1987046068139414
205;0.3936063654860071;0.2127307707868208
210;0.46132956278614545;0.20611444264132725
215;0.40960771168080495;0.2102384303720333
220;0.4946514699711483;0.19386542528762798
225;0.3699570279119865;0.2410190161401095
230;0.42038660065312383;0.1988226382868441
235;0.35446249706155064;0.3118336517364394
240;0.3555938994939996;0.2872035979128933
245;0.3973063779282045;0.21139153728349955
250;0.4957943296480619;0.19286471589974982
255;0.3931713704625395;0.2111951607289327
260;0.448654696252111;0.1987444778786863
265;0.3835133412258945;0.22063433805414354
270;0.37743642032785524;0.23062408594176612
275;0.3729623461290233;0.23253859893453352
280;0.40362871038884496;0.20925894105925874
285;0.3873824372313694;0.21898575688920932
290;0.4545411379793808;0.2036136323102599
295;0.38921127927095683;0.2144621818060526
300;0.48045705294701396;0.18985005914460112
