Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 256, 128, 64, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.32238296985870757;0.1355563285066093
10;0.3360486806382422;0.1432941427484523
15;0.31967749259932454;0.13805839054320596
20;0.2649134324730918;0.11805568263772981
25;0.48916617703230314;0.1455525470491935
30;0.29330196880508996;0.135522034183862
35;0.331839548269287;0.1342111327653481
40;0.3102572980213223;0.13835563821530752
45;0.3389380815745487;0.1254885521763255
50;0.3412990362796385;0.14186840189362235
55;0.3575555307005971;0.13058760654093948
60;0.31945427741189764;0.1375638156005478
65;0.3154875649170963;0.13874130514449146
70;0.3209153281205751;0.13767379065249474
75;0.37359028153566465;0.13183760501605774
80;0.2803644246475448;0.14953654863978172
85;0.32628168495594917;0.12946507715543787
90;0.27022586439652435;0.14452295479701247
95;0.3621609492955332;0.14633215623459184
100;0.342574023700071;0.13304187802598738
105;0.4051518044452185;0.13293323815390534
110;0.32654563822345634;0.13541960268590691
115;0.3052958793006992;0.1338895658976317
120;0.33014168390126797;0.13267312816603508
125;0.29092132464852066;0.14022765438442433
130;0.40208042975937625;0.14602492112149695
135;0.36062508863017534;0.13280765718444898
140;0.35053266312480436;0.1364509735450152
145;0.4386263829113255;0.13204701588634457
150;0.28398403330154315;0.1372920573503963
155;0.29333645263223473;0.12736081973355529
160;0.40387881512803925;0.13578848839589963
165;0.3263733512293423;0.1321101119345176
170;0.24649576607666415;0.17698584932880115
175;0.3104846549818518;0.13401696165253577
180;0.32273977417912053;0.1405631962118507
185;0.27906787098265534;0.14672466868531983
190;0.2870092405483191;0.13600990643141128
195;0.3272305850324454;0.13687906469336797
200;0.3486405639420845;0.13384943727121784
205;0.32573238240250685;0.1392199522212678
210;0.2951399559988283;0.1431892580111076
215;0.3052623013641167;0.13587433117913555
220;0.3371828633904797;0.14068342820258842
225;0.27721244429473246;0.15107651820081924
230;0.30175880727445603;0.14120772591327788
235;0.32514057210671327;0.13968131901015782
240;0.3436861246091088;0.12989728523256358
245;0.36482027111895143;0.14328786003646132
250;0.30499675737727117;0.1413089084728404
255;0.3075315733657686;0.1304397957651092
260;0.2520663218841776;0.1391547541762289
265;0.3176862016090018;0.13879604745346147
270;0.3798743560327893;0.14142316308591793
275;0.3263147806662237;0.13346906126086888
280;0.40949420814841964;0.13715423935034346
285;0.3481904956274826;0.13801107634947532
290;0.28612259591322065;0.1434834704514457
295;0.2814745707445727;0.13988531596577197
300;0.3186932782572496;0.13881548050073397
