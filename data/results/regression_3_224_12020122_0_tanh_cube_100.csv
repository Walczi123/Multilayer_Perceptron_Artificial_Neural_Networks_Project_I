Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 128, 64, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.4092746173261501;1.3382992016013466
10;0.3661807342898155;0.3572978662658274
15;0.3904191879096074;0.25178702908734
20;0.3852780684166005;0.7148567776248623
25;0.4512241801428444;0.21861147779396256
30;0.36857110847625785;0.3759543530291505
35;0.39138751985358766;0.2465698110872082
40;0.45041502183315085;0.2290179598771033
45;0.4452577565611655;0.1984418510489076
50;0.3707163301093663;0.31154379441843955
55;0.43693945766692543;0.21167018652532224
60;0.3968126258737418;0.2737229446919906
65;0.5316858741893483;0.1962542388451468
70;0.4828301891664529;0.19433893902286367
75;0.4218649165267999;0.20702204760859444
80;0.47938352679514773;0.21009509191781264
85;0.6409270774438087;0.15913337616575896
90;0.3722276478348471;0.44794759381598787
95;0.560024236672484;0.19323261118572876
100;0.48030319316896863;0.21484725464713153
105;0.611170891076701;0.19980933159040004
110;0.408711318749822;0.22882436454406288
115;0.4280665459879873;0.21933461179080627
120;0.4499682897980586;0.21578508768774524
125;0.37204438989608146;0.4766178380843926
130;0.42388533563620356;0.22422723653456872
135;0.45433062693529885;0.2042455843009094
140;0.38423255855632116;0.30343457580077293
145;0.368496266624478;0.29526366991475655
150;0.3934710737490014;0.26169585009185137
155;0.42128801589609366;0.22854153124764404
160;0.4874173501684118;0.21678471738731495
165;0.3722615586127127;0.5017461315599469
170;0.4443538342255983;0.21364978463158416
175;0.41209273876003305;0.22918460115481679
180;0.6815994722195814;0.17688234959259
185;0.5869418275148452;0.20016031367795856
190;0.4452733987282129;0.18020779751708435
195;0.39709382736057536;0.21670241768714046
200;0.4446034378047254;0.20895332637862463
205;0.5306110553445151;0.16098583840125227
210;0.4754091401667796;0.21630022949905295
215;0.43912624242028625;0.21987865109956733
220;0.5430152720742765;0.20908161962102675
225;0.4247771076675828;0.21201669929910333
230;0.42089856105280554;0.21572942291947628
235;0.3729384171412383;0.5071395473333747
240;0.3650561340576778;0.3648306186182919
245;0.42917473062454337;0.18104929651342483
250;0.4509634860567225;0.20607593592410603
255;0.4063210320921658;0.2261806988232001
260;0.47816876069103925;0.21423146829828574
265;0.370460121830332;0.32773103133978043
270;0.43492493132145227;0.20254223118846038
275;0.3993981108280972;0.2076846558818696
280;0.40141714376513155;0.2477511071266921
285;0.4174127920668865;0.21991818900979015
290;0.4474275687160446;0.2302313600864494
295;0.44578689274277217;0.19375236437486462
300;0.4658148621906916;0.21737639879943574
