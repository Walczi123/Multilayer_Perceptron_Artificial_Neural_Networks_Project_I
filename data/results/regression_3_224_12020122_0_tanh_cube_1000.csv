Train dataset: data/regression/data.cube.train.1000.csv
Test dataset: data/regression/data.cube.test.1000.csv
Layers: [1, 128, 64, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.41332376761213974;0.22471223478023825
10;0.4454323042667497;0.20725518947198077
15;0.4431423996613873;0.23027758559033984
20;0.525663467644908;0.19453964063739745
25;0.4822815744057242;0.21316862587657306
30;0.42238365532553024;0.2198013883431297
35;0.38263084221532223;0.30179379725125033
40;0.4463129194308377;0.22268995411776535
45;0.4791889710758147;0.26231866503325485
50;0.4980734815221308;0.24286110239263733
55;0.49279955740247383;0.1612131779021438
60;0.42072120640308025;0.2429608768313795
65;0.4358360546198586;0.22691826515746624
70;0.4491723706980727;0.23021556166545581
75;0.45195503556378636;0.2215026301134354
80;0.499642732573452;0.22581898661493754
85;0.4902795976403245;0.19608373473824972
90;0.3899173578476334;0.2356574204611255
95;0.44939619904677375;0.2205332778570414
100;0.428288723753667;0.25807111314967096
105;0.41197482985865663;0.24888401740199217
110;0.3790651198674365;0.424737666951275
115;0.3967717593136454;0.23728083857392238
120;0.4141315034567592;0.265743082001125
125;0.43873796212097704;0.21817404397202794
130;0.48655358591242054;0.2705790596754652
135;0.464738612281396;0.22381064713784557
140;0.5485623513966335;0.25751947928808555
145;0.4624604088956643;0.20781615312437934
150;0.5944344223414886;0.19128356863142829
155;0.4530986679063797;0.21435318420684865
160;0.40780591871338073;0.2572856018041179
165;0.4070741117973535;0.2263353270861702
170;0.36984651643876104;0.27842976565209265
175;0.39328533927094544;0.34070471415231507
180;0.4558455670873204;0.17643545545984815
185;0.46288670408054355;0.26030919924703033
190;0.4325141829331703;0.24541193254171548
195;0.5347396431854856;0.21804309271514702
200;0.48365728512019257;0.2134323299706408
205;0.5059138584465013;0.22280521954468072
210;0.44717320007394196;0.21089776677256356
215;0.4680919572786117;0.22687690557541274
220;0.44219497866988455;0.24402562840950331
225;0.6013565801914043;0.2636248293880144
230;0.38019679301563186;0.5746657512568828
235;0.5143293918645362;0.17555628105355825
240;0.5956110933350508;0.24283635866360226
245;0.4113715852816069;0.2300323142508746
250;0.4850001403502925;0.23489259399866722
255;0.41146747675330625;0.23284053906960397
260;0.47133464605578995;0.20003611912019306
265;0.5860560882740664;0.19720745850213206
270;0.5321902349416633;0.17330835266326733
275;0.46281576198227714;0.2273846382264291
280;0.39631477186283215;0.2642749550492248
285;0.39743628078190196;0.2515007857003981
290;0.4510516850914028;0.23309175927668127
295;0.3912158254336457;0.26806867478072266
300;0.5466948462372655;0.2268061019856836
