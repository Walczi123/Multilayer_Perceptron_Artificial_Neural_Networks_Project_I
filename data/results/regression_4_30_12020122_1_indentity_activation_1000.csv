Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 16, 8, 4, 2, 1]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: indentity
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.01777879833837753;0.03408012339916162
10;0.01982494147007447;0.03076387515746688
15;0.026662985622474253;0.03254789918562923
20;0.0481377605627459;0.03769379498538898
25;0.025047511157124704;0.03070912873043109
30;0.049749726945596554;0.03726933464271286
35;0.014741310320387571;0.07711378653954648
40;0.020281931935345544;0.02958964379888975
45;0.024546808403562225;0.03140266890970848
50;0.017832121686734627;0.07390657592733069
55;0.025817832039085945;0.030887079251690666
60;0.019813887653285508;0.0298888394213695
65;0.048566217031726054;0.03744341897434265
70;0.055095659573673045;0.038310263905549743
75;0.039577844562079904;0.034550854974234824
80;0.030525828647096787;0.03235664376680197
85;0.02144745718186957;0.02988731748757452
90;0.034828217649019264;0.0332032293052156
95;0.017741166425704464;0.23494062663262583
100;0.044850145902233615;0.03645455470500105
105;0.021569147176226756;0.05508337359718257
110;0.016696805908568205;0.03571682381843007
115;0.01738712556564763;0.05953379547567634
120;0.037788637450253654;0.0341010819893826
125;0.026602260015267217;0.03192835944567615
130;0.02234989579409995;0.050147618762656575
135;0.01701557256710585;0.04022134181180189
140;0.022942147688054682;0.03332451088886967
145;0.03114191844245504;0.03413953055854624
150;0.03364388573716504;0.03311643236321115
155;0.02164600354836357;0.029861328258617075
160;0.03610842730092073;0.035105197289557785
165;0.03153894016963895;0.03237943659987406
170;0.02369713862666086;0.03050773456361534
175;0.029469492748363;0.03220238608685856
180;0.05349343851225991;0.037668147967600894
185;0.05625166091714167;0.038599723588484174
190;0.03488971256806779;0.03400131030026793
195;0.02700367087131418;0.03116000052489102
200;0.026429266524583946;0.031135429106166304
205;0.029779416170305672;0.03187449511084492
210;0.026564108625710292;0.0313915664202641
215;0.01846126243417296;0.03034957467621119
220;0.016893375912162455;0.04208754418836692
225;0.025929322650945597;0.031111447940808
230;0.026123103492242585;0.031591280751684124
235;0.035939546898127346;0.033607312097547574
240;0.020454617426829916;0.030365033339179705
245;0.025360184184925215;0.04484458682342065
250;0.021768798580973237;0.030149488218718185
255;0.04439457389078373;0.036817025258117156
260;0.022254974094374047;0.029995780752959576
265;0.026651452856531466;0.03122073447414465
270;0.021690492491209773;0.035565068352621014
275;0.04787502206357702;0.036417961541090726
280;0.0403799216832531;0.034941633718961196
285;0.0415560632367052;0.03487570465035346
290;0.037931851577500356;0.0341560534299578
295;0.04746519203140034;0.036320466323726146
300;0.03428191820608822;0.03368175546142316
