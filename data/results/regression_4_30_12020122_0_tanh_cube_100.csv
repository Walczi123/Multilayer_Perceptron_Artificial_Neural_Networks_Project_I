Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 16, 8, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.39390987492615137;0.4580977680100128
10;0.37667069302379963;0.24681267511941415
15;0.4138753667660801;0.22800235564247076
20;0.37160176996733113;0.3832796402744829
25;0.42985719781495374;0.24642047014601304
30;0.3882534007767725;0.22455540942844998
35;0.4539298016585753;0.24123901469476758
40;0.4464547118152828;0.2534525690264728
45;0.41250131778340815;0.2419529129380023
50;0.37867922724722214;0.2311080756174551
55;0.427776927831273;0.26397830406339434
60;0.426336446982897;0.2649030994717416
65;0.47637720102715625;0.2428718783253617
70;0.44480554406682815;0.23563032324100827
75;0.4160481589373426;0.23102241530132264
80;0.5026967760930057;0.26636784155399673
85;0.5006462555955878;0.22974275800790042
90;0.3939946631488978;0.25443135519388577
95;0.4922890859938412;0.25869497111226136
100;0.45044789222268217;0.2502605972246992
105;0.5019404070716917;0.24944374102684036
110;0.41657397673945146;0.2663028286583665
115;0.4125722555548231;0.2446037797158013
120;0.4623886040112462;0.25353987195149563
125;0.4022848843820404;0.25703207270429695
130;0.4272801195647009;0.23370286605591886
135;0.46586979494757697;0.24918281489754615
140;0.4171434091605404;0.2406628010848982
145;0.40542180846756326;0.24192331362617664
150;0.4379723541842783;0.23645294623829044
155;0.4299577554772248;0.2442297721952404
160;0.49373834352356516;0.26503366128133765
165;0.39669536762289714;0.2451745662536937
170;0.445557338103725;0.25156384658897885
175;0.42254868519327565;0.2419550740364485
180;0.4441549653877447;0.23385091462774038
185;0.46439344611068545;0.2500080420279588
190;0.41906043904008194;0.23175714624178528
195;0.44416141941064446;0.2347486462157616
200;0.4421950814774626;0.26929015719766564
205;0.5050501363347598;0.23577189767868548
210;0.47042482639270905;0.26141302716084774
215;0.4413418920118914;0.24969514601587672
220;0.48227987407225514;0.2484793569212782
225;0.4095092844863107;0.26970038047260064
230;0.4551661696761246;0.24566175714159225
235;0.38865439884389724;0.3222284531052918
240;0.4090314980634104;0.2405578026138572
245;0.4098681994963124;0.22282505914177875
250;0.48023312496773923;0.26635057359302805
255;0.4618478785272053;0.2587003594332772
260;0.4381917860745692;0.23904720184228304
265;0.4276966447677812;0.2432875352813275
270;0.4626090347225377;0.25459300380269234
275;0.44107067067111416;0.2504471471687847
280;0.44291976351799334;0.24795227288380078
285;0.46521651051736806;0.25792485912755037
290;0.4523537552278942;0.2755193544121521
295;0.4530923841163598;0.24988753202538114
300;0.4859061222931222;0.2610101091948542
