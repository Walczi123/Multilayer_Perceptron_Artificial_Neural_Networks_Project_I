Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 64, 32, 16, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.3689127598112314;0.15792716194191797
10;0.23603264687551312;0.14333134052472538
15;0.35174281651471406;0.13217707519848296
20;0.26689248685636535;0.12546859922881443
25;0.3593249790736674;0.15046143090812772
30;0.3013232757174228;0.11887218616185856
35;0.36649767195242805;0.14648133295759908
40;0.2605128125425833;0.13330168945235132
45;0.3271283417211871;0.1119487656497696
50;0.2718035457371212;0.14842557320676514
55;0.31712184517709235;0.13064673405653435
60;0.3487452196464868;0.1356084768907207
65;0.32394663168159876;0.13386941865710994
70;0.3023308584243232;0.12097078692013327
75;0.30575103594714764;0.12211926548715865
80;0.30378152663039043;0.11756440809204761
85;0.38249752791314606;0.147974731268852
90;0.32489331286842327;0.13932791952165371
95;0.2788692930542071;0.12242091494271715
100;0.332742437824813;0.133623998616429
105;0.2762057164636958;0.12269687849462001
110;0.30268016874409864;0.12542628685844584
115;0.4163860289411525;0.13420651688410792
120;0.2907420239639411;0.11927476047476211
125;0.3179547953992042;0.12651016900607037
130;0.36629154541223985;0.1253081098450831
135;0.2930428816660664;0.13590510347311469
140;0.33255556949094356;0.13729635901834825
145;0.4040965605850181;0.1463297931837068
150;0.3417422468050926;0.1327313476235319
155;0.2978428453599707;0.1239160071589718
160;0.298591170890733;0.1394773691109322
165;0.34866080688350776;0.1323385183112345
170;0.32821888998719184;0.13212668093864088
175;0.32273223948229035;0.1308879493257747
180;0.288412560833827;0.1286764007318399
185;0.31673552724107373;0.12232340088801781
190;0.30616482028574765;0.13147170891250767
195;0.3175145659918794;0.13957423325365936
200;0.34333095553982823;0.13212959221990395
205;0.3786265902174326;0.14390289385794466
210;0.29623377775689314;0.12506096435893838
215;0.33412224301299254;0.13696134631850565
220;0.3246900546974386;0.13363064592242258
225;0.29761773748310416;0.12908285669147335
230;0.3415553862569975;0.1308001533039193
235;0.37761189313352284;0.13186514386538575
240;0.3443386233026583;0.1410206577619545
245;0.31456019872722285;0.12650625555070732
250;0.3510793026465664;0.13733253571985035
255;0.2723596426793563;0.1311361949017163
260;0.29985650873868963;0.1347108335033084
265;0.34344892275413197;0.13260643205375533
270;0.2583544867725997;0.13310340867324907
275;0.33493260656840096;0.12918829147690067
280;0.34655967685717276;0.131625216681649
285;0.31627111240534467;0.13285602609415537
290;0.3063404636692643;0.12406378646425448
295;0.30139253840729663;0.1342070461669437
300;0.2768241318349347;0.1362326619018213
