Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 128, 64, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.36300086393813125;0.14962361759866183
10;0.23742731140216908;0.12186843329340633
15;0.3631408987829027;0.12512510257017093
20;0.28182378794974494;0.12816347062763941
25;0.35495407748144986;0.14962578111229669
30;0.3050224931887942;0.13649813079965692
35;0.37069109491412044;0.15266701192555432
40;0.2693314781048738;0.13461504514038558
45;0.3289195033808721;0.11696164515971029
50;0.2972053634907341;0.14470509349154692
55;0.3276564573022631;0.13585143575986164
60;0.35262241328637806;0.13212023958211083
65;0.3299058639067218;0.1395937049598007
70;0.30540100479852605;0.12229937771441389
75;0.31767363322318737;0.12106219456747029
80;0.3102331943207548;0.11610570139020415
85;0.376962849248731;0.15073846924694245
90;0.3259640439441884;0.12855291797658544
95;0.30167049982010924;0.1203329528064188
100;0.3290670157802861;0.13673996243127198
105;0.3126417372632558;0.10483238070327054
110;0.3084018580444072;0.1251067349821585
115;0.3884049685197316;0.13811494021365178
120;0.29322237009313307;0.1232573776507488
125;0.3100343520859453;0.12801121043271266
130;0.33675570092020873;0.13097581364964464
135;0.31422534451664774;0.1337341927474166
140;0.333225600612382;0.1343159018453021
145;0.39049413087190465;0.14930684232159802
150;0.33338041373584615;0.13052968903911666
155;0.29819828507512114;0.12426776862895626
160;0.325185827219068;0.13401466404875664
165;0.35328949514244357;0.1278230761839768
170;0.3213603717597598;0.1294428422042238
175;0.3283056624519983;0.12947447176654356
180;0.30986734705479435;0.12905561369343654
185;0.3176454067177323;0.11991503842386449
190;0.3162652805130472;0.12412348925697313
195;0.34755340575306654;0.13528161837123368
200;0.3295459817064394;0.1380527794551791
205;0.3839752808173956;0.14536382580898224
210;0.31840198321192653;0.12315140164973853
215;0.33045317736547214;0.1364430876784192
220;0.325069998180258;0.13283788767167534
225;0.3066832615002887;0.1248226353356795
230;0.34098795901566553;0.13280825058520127
235;0.37060996812447783;0.12747026666242564
240;0.3575077142052884;0.1427298221441102
245;0.3118678266127522;0.12561125994011338
250;0.34742220179619643;0.14339229136617396
255;0.29158775845163043;0.12637306896860542
260;0.31582504650971316;0.1292514108614886
265;0.3397359108157445;0.13091786376258469
270;0.27707656019008803;0.12396608209235395
275;0.3401794693556677;0.13747971194208436
280;0.3221206952734342;0.13234473008363742
285;0.3069366039151379;0.13398809315715662
290;0.3022124237537225;0.12058990047222731
295;0.31128788742812513;0.13075470308953507
300;0.32043078778429973;0.1255362177803306
