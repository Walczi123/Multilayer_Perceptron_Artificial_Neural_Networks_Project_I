Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.31646936790567254;0.12283345652560093
10;0.27394744492403766;0.1180715202218695
15;0.3057164299599297;0.1292231587430391
20;0.24544318100702206;0.11267568106634249
25;0.362050092232574;0.12259174911835066
30;0.2857382815592064;0.11123028334603972
35;0.2700459709681083;0.11518911300979963
40;0.2901939464157766;0.10401817886488489
45;0.31516038782668815;0.12735192436750176
50;0.34134306719814317;0.13630985387811287
55;0.30382693784998915;0.11262752793422844
60;0.3299421383844955;0.1165686354662501
65;0.3292488224775214;0.11699327940661097
70;0.32466742895683676;0.11455067031803216
75;0.2950643924387591;0.11891018659885104
80;0.3022342083699176;0.11528886325566336
85;0.29088447112751575;0.10434045815863334
90;0.3325104401917058;0.12471672237091849
95;0.3742800534096465;0.1553661678286114
100;0.36141911693061735;0.12485585984953722
105;0.3674535086626107;0.1392400120824744
110;0.30581518534901797;0.11725756082248724
115;0.3160161838895564;0.1421908868691047
120;0.33828161828841596;0.11489973965456209
125;0.3538411835987515;0.1246028829571248
130;0.34424485774287283;0.15360849843014346
135;0.3102303379029098;0.11977825731892615
140;0.3423659851357195;0.13560838886574897
145;0.3735437845699058;0.1410071774128057
150;0.30099530104913713;0.11739464589221613
155;0.2751029886811228;0.10917037184487001
160;0.38738527437905645;0.1410595092254075
165;0.32152804531731527;0.11782304100416238
170;0.2957209947019535;0.13466527131764272
175;0.2881641960003901;0.10989120837449974
180;0.2896273048144018;0.1375196559663173
185;0.28455306125803354;0.12004831075791346
190;0.3010862630321449;0.12167864308005619
195;0.3291776680093604;0.12302219267437206
200;0.3246910567830567;0.12215046313739669
205;0.3292311570244301;0.1273641714514321
210;0.3424653574094299;0.14392558184347787
215;0.33043480677623066;0.12399852600046952
220;0.3340536918917737;0.12830441107590745
225;0.28367163570232495;0.1207444434537346
230;0.3066603981410794;0.13119757663935586
235;0.3489743330059675;0.12649173897271496
240;0.31463103414602744;0.11387734662302224
245;0.3377492544006661;0.14053121461008525
250;0.3239420615673038;0.12458117033033785
255;0.2736714728910474;0.09856971043964337
260;0.273430912302217;0.11704428674339305
265;0.30994333382966327;0.12503193779966532
270;0.3468540692245552;0.12742413247176598
275;0.3484762531181294;0.12991957024038572
280;0.3520922608059925;0.1365492565560417
285;0.3940866922067464;0.1268665953590782
290;0.3259469575966167;0.11743997604757735
295;0.27806265732120833;0.1192559112949562
300;0.3548912193700791;0.12594277621375283
