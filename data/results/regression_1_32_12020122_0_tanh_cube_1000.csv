Train dataset: data/regression/data.cube.train.1000.csv
Test dataset: data/regression/data.cube.test.1000.csv
Layers: [1, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.6998640002849182;0.16332418200661572
10;0.5718480508208806;0.15374614485845156
15;0.5872497893784079;0.15813211179275907
20;0.4266013766935828;0.19292459897048397
25;0.5326807673482535;0.15896335767112135
30;0.3628445351099214;0.32511553463927473
35;0.40271321770878044;0.20024199754008
40;0.3926745111803731;0.19793476042284483
45;0.46810515204321207;0.1809764044752966
50;0.5098809262950731;0.19907255439164653
55;0.3687524675356133;0.20313288482794903
60;0.6622929958811978;0.17074150218364637
65;0.3811735575038516;0.22489428391361416
70;0.5188960062603716;0.1692507359368663
75;0.4755525316866505;0.18569719241139243
80;0.5525216680078441;0.18116761579943222
85;0.47983526777263685;0.17309678630332892
90;0.5410650380604439;0.16279276820791536
95;0.48254662555464495;0.17597709735123102
100;0.3897647528645377;0.2256975752028604
105;0.38809924300118304;0.20640620834496212
110;0.377957192045056;0.2501668841584464
115;0.3712305221206579;0.20236272059475638
120;0.8335699437547408;0.16770340938418332
125;0.48774827038493573;0.1616913677287296
130;0.49218239814678727;0.1974793170510176
135;0.4134047046185973;0.19218868023756222
140;0.5743088013689228;0.20976994415199657
145;0.4468231870845053;0.18294832730559632
150;0.5689476052174945;0.16666728921112653
155;0.37125044825366216;0.21235875044588431
160;0.45067405972145236;0.17871357514597597
165;0.4483218998794787;0.17471604001784302
170;0.36828753991726504;0.38262436070924843
175;0.40340500447538374;0.22783238326516986
180;0.364331072426753;0.2025194530878853
185;0.43105797716667155;0.221307302603505
190;0.5313429210154541;0.18581410138539164
195;0.5531577580184389;0.17532332697713665
200;0.5343164784327259;0.16848018180518443
205;0.505728560616388;0.18637850819525462
210;0.3706285467198651;0.21254169982238322
215;0.4873546128729596;0.17602563630218532
220;0.4922819674229736;0.18202029193184213
225;0.6165667248336245;0.20948309476284774
230;0.36536482091299577;0.38926857860298547
235;0.3661319350226129;0.22478943958509093
240;0.7352795209414505;0.18217086223586107
245;0.43896519881347307;0.1759376034743132
250;0.6230140818926235;0.1814107464737568
255;0.41028766337141637;0.1850697224464943
260;0.367959388060742;0.24931162405559612
265;0.581808432750772;0.1719023504226521
270;0.3702321631828428;0.17903901509205922
275;0.5007095829913156;0.18241588737900344
280;0.36084490679397274;0.3217478233917881
285;0.40406088627338876;0.1857748712360212
290;0.544638681403754;0.18364624525673176
295;0.369546494740585;0.39674294306467384
300;0.6456884218528602;0.17628102670083123
