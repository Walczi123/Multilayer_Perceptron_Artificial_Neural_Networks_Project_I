Train dataset: data/regression/data.cube.train.1000.csv
Test dataset: data/regression/data.cube.test.1000.csv
Layers: [1, 64, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.4088570922352046;0.2203403223972088
10;0.49284374659205854;0.1845857467310611
15;0.45891537257296117;0.20623253766984512
20;0.5197714111939101;0.16985076965073606
25;0.5062049857364829;0.18740249984854573
30;0.4465310604611059;0.17988193483929196
35;0.39496342266825357;0.23264815205978312
40;0.4369720349632468;0.20876509896275583
45;0.45578192327233735;0.2756797698418729
50;0.47497519406766564;0.24342445893952064
55;0.4949702877673313;0.13956507701432888
60;0.42033073773513174;0.23477301761177008
65;0.4132307893985682;0.22987989371407022
70;0.47023574123302103;0.21007587016148427
75;0.45123401301312577;0.21854985535383398
80;0.5078056034701096;0.21410361087243812
85;0.522445348019859;0.1732673479710717
90;0.41835116976480863;0.18718795118858067
95;0.4888990276656835;0.18445442402240947
100;0.4048872222059229;0.2822340574420622
105;0.3982986435830033;0.25577402064880966
110;0.4042741798669595;0.2948112581156195
115;0.3863429374774425;0.23759841612219887
120;0.42523952949009275;0.2577901989559908
125;0.44528055053561216;0.2078329867867579
130;0.48965147822349087;0.25973066580683113
135;0.44488554967134486;0.21115469588118804
140;0.5259238629402001;0.2574739411726665
145;0.478403454839431;0.18685990087318502
150;0.6156926770811089;0.17634191824435788
155;0.414809063363841;0.2030806161183336
160;0.4414065680954905;0.21494720101754863
165;0.4059766921913071;0.20544191094558073
170;0.36780557858625956;0.23248354309447483
175;0.4531745257349947;0.2597463353186904
180;0.42323757510760246;0.1445237264970153
185;0.45536406803692386;0.2556428291485207
190;0.4578021517347874;0.21644463761859845
195;0.5473097495077147;0.19851459444803604
200;0.4793325572325183;0.20535685367618314
205;0.5232415183566373;0.19010973875670378
210;0.41926996779391873;0.2032113257010861
215;0.4713413460702426;0.21739721901479656
220;0.4862156481239908;0.20899681206859766
225;0.5477110345508542;0.26390006111350106
230;0.42866633966191653;2.2742244972833365
235;0.4916416205287711;0.14880207071368837
240;0.5505461970148059;0.24243062647839086
245;0.41809757434775036;0.20792327937747726
250;0.5342765776416746;0.2110546000422348
255;0.4170717830167272;0.20767002961102188
260;0.4587566541052734;0.18359593985714678
265;0.6075534530291576;0.18012116631897687
270;0.5125204829352804;0.14480174405540736
275;0.4582881172070806;0.21163308333728886
280;0.3934314962873468;0.25732654407476546
285;0.41095106042990903;0.21922977033461805
290;0.48505001381694324;0.20574792006596165
295;0.370750232452478;0.3644335750140785
300;0.5999706775875326;0.199912984554845
