Train dataset: data/classification/data.three_gauss.train.100.csv
Test dataset: data/classification/data.three_gauss.test.100.csv
Layers: [2, 16, 8, 4, 2, 3]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: softmax
Problem type: classification
Epochs: 300
Epochs;Loss (cross_entropy);Loss (hinge);Accuracy
5;-442.2519244758277;0.08666666666666667;92.66666666666666
10;-443.9259007760661;0.08;93.0
15;-450.9751551339886;0.06666666666666667;93.0
20;-456.12841827040506;0.06666666666666667;92.66666666666666
25;-450.1642249511056;0.08;93.0
30;-450.68747309487014;0.06;93.66666666666667
35;-451.49840327775314;0.07333333333333333;93.33333333333333
40;-450.5696900425471;0.1;92.66666666666666
45;-417.38930846123594;0.06666666666666667;92.33333333333333
50;-457.22703049240647;0.06666666666666667;92.33333333333333
55;-463.1255566938553;0.04;94.0
60;-427.3946015115721;0.06;93.33333333333333
65;-427.66873236731215;0.09333333333333334;93.0
70;-458.6133247535264;0.05333333333333334;93.33333333333333
75;-429.0029106939596;0.09333333333333334;92.66666666666666
80;-476.086888436605;0.09333333333333334;91.66666666666666
85;-453.2380466958422;0.04;94.33333333333334
90;-496.4893884453403;0.10666666666666667;86.66666666666667
95;-445.0245129980675;0.06;93.33333333333333
100;-447.3916364288658;0.06;93.33333333333333
105;-459.35858781855876;0.07333333333333333;93.0
110;-449.4189618860733;0.06;93.33333333333333
115;-441.7286763320631;0.06;93.0
120;-452.71479855207764;0.06;93.66666666666667
125;-466.8789743857735;0.07333333333333333;92.66666666666666
130;-447.96700050710274;0.08;93.66666666666667
135;-444.9588458802168;0.06666666666666667;93.33333333333333
140;-440.5643969922111;0.06666666666666667;93.33333333333333
145;-442.70950550174155;0.07333333333333333;93.33333333333333
150;-447.74498558583497;0.04;93.33333333333333
155;-445.429978089509;0.05333333333333334;93.33333333333333
160;-441.20542818829864;0.08;93.66666666666667
165;-467.284439477215;0.04666666666666667;93.33333333333333
170;-452.71479855207764;0.06;93.66666666666667
175;-455.31748808752207;0.05333333333333334;93.0
180;-459.7640529100002;0.04666666666666667;93.66666666666667
185;-453.8134107740791;0.06;93.33333333333333
190;-452.71479855207764;0.06;93.66666666666667
195;-461.3859132757662;0.07333333333333333;93.0
200;-441.0876451359756;0.04666666666666667;94.0
205;-439.8712498616511;0.06;93.66666666666667
210;-441.0876451359756;0.04666666666666667;94.0
215;-446.5807062459828;0.04666666666666667;93.66666666666667
220;-464.733865876243;0.08666666666666667;90.66666666666666
225;-459.4242549364094;0.06666666666666667;93.0
230;-481.63206548108457;0.08666666666666667;90.66666666666666
235;-455.25182096967137;0.06;93.0
240;-448.72581475551334;0.05333333333333334;93.66666666666667
245;-431.3043670069073;0.12666666666666668;92.33333333333333
250;-443.1149705931831;0.06666666666666667;93.33333333333333
255;-441.0355292015032;0.05333333333333334;93.33333333333333
260;-446.9861713374243;0.08666666666666667;92.66666666666666
265;-438.60273865285427;0.08666666666666667;93.0
270;-440.74784716238474;0.06666666666666667;93.33333333333333
275;-443.808117723743;0.07333333333333333;93.0
280;-442.99718754086;0.06;93.33333333333333
285;-447.10395438974734;0.07333333333333333;93.33333333333333
290;-447.10395438974734;0.07333333333333333;93.33333333333333
295;-446.29302420686435;0.06;93.66666666666667
300;-446.29302420686435;0.06;93.66666666666667
