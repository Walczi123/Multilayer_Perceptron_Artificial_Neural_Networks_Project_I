Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 256, 128, 64, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.4624008663340112;1.2291239223339505
10;0.6320396402796471;0.49983348642807823
15;0.36964805668166595;0.29788605843610816
20;0.38367627240392616;0.668993018051843
25;0.41038918889636966;0.20427117023600785
30;0.37359900558786496;0.32578334034522644
35;0.43815493846577247;0.2539592099053065
40;0.4295837649410128;0.2526592814514308
45;0.45681900594565106;0.22637010900978774
50;0.3641087652521908;0.2854477309967553
55;0.4797001551353612;0.2412983921745711
60;0.4339353613438996;0.2638055588203526
65;0.4769279251385273;0.25642177899285024
70;0.47585804372423834;0.2404051883302427
75;0.46094645020857755;0.25026366556253876
80;0.4979738247814239;0.2535527383693646
85;0.6174450233324144;0.2068841976045892
90;0.38141571643060207;0.6081460988263978
95;0.5783486418560709;0.21311694198856312
100;0.5499297975166494;0.19349008451720245
105;0.5852362924388578;0.22376376499274417
110;0.4584674342044907;0.21929261601017
115;0.5105858973934279;0.19378439300240385
120;0.4839151767409211;0.21042383216121086
125;0.3766203313833385;0.4541292351801715
130;0.4397424116431498;0.23212507225313952
135;0.467624621596403;0.25147785285477037
140;0.4734106968015086;0.2422986094092806
145;0.4247562989709167;0.28615862669148145
150;0.45970147159418184;0.25535670839097585
155;0.46794171516922434;0.24433866743046131
160;0.4835171578280081;0.262294113747145
165;0.39535197967690255;0.30290244899907415
170;0.4570751461317164;0.24889139630866838
175;0.42717289351891685;0.2142460003873281
180;0.5423223029186596;0.21063102216752502
185;0.4854955484105696;0.2445778031831394
190;0.4210839096556363;0.2022182313094056
195;0.4579231715683167;0.2399334281084421
200;0.4660578228986667;0.26744164170262075
205;0.5453852901642032;0.22657420198736025
210;0.5283925091305898;0.2999104790756941
215;0.5043298788103417;0.24207003744323707
220;0.5328883887862897;0.2604556888375691
225;0.4339116966162837;0.265616713001418
230;0.41731474632575055;0.2366242891805651
235;0.6063654463283834;1.5584878753635798
240;0.3876481424903819;0.26753408285112573
245;0.40379823893997463;0.24326649557512606
250;0.474496329654257;0.2821506139384875
255;0.45545627143663225;0.2573233995985697
260;0.4822057451230612;0.2344799386775569
265;0.37880114472124415;0.337887192828221
270;0.49619480634089275;0.2338891648926291
275;0.4582723853106002;0.25105262575052223
280;0.4657379379323298;0.23638598232905092
285;0.45624218402291833;0.2712691569175541
290;0.49382231230623297;0.2664478397875397
295;0.507967508883142;0.22724446458426484
300;0.4474405559401521;0.266236571744647
