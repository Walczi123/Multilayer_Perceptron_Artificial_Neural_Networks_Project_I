Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 128, 64, 32, 1]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.00021836616737365582;0.033382770914556485
10;0.0003286308462186207;0.03677765537465739
15;0.0003193133344354086;0.036836222462210765
20;0.0002446118377624895;0.03447341907502098
25;0.00023626498010521658;0.03430021751411433
30;0.0002135158921620927;0.03335022417992584
35;0.00027028463556881565;0.03556947475485372
40;0.00022849937688583798;0.03428840668695561
45;0.00021328056728338367;0.03365514783794159
50;0.00024435200546244526;0.03492322200920097
55;0.0002366754124806351;0.03461815685950452
60;0.00020756651185877445;0.03359762961654321
65;0.00019337302150593881;0.0329704030460062
70;0.00021322519710314166;0.03396658260642133
75;0.00020420528629965936;0.03361600572188522
80;0.00018459093226042057;0.03275383657633882
85;0.00018833191264257772;0.032959251640628094
90;0.00019548207401837517;0.03343933001603995
95;0.0001747431030799801;0.03242409938695513
100;0.0001974638585668215;0.03355394551720198
105;0.0002164825547920518;0.034493336629067846
110;0.00019502504063591383;0.03351909129993519
115;0.00019598862555814407;0.0335863712549643
120;0.00017652557932394287;0.032708122222278264
125;0.00018862706987136897;0.03342012909889423
130;0.0001689539387836676;0.032411297796282684
135;0.00016580575270939716;0.03225012148977284
140;0.00018912065650177873;0.033572372920459406
145;0.00018975295515433058;0.03349569944668433
150;0.00021194424337863717;0.03448548035164101
155;0.00017964776262034628;0.0332057604615418
160;0.00017485995278160104;0.03285255790356594
165;0.0001774920584714407;0.03324044044560309
170;0.00019477143411234963;0.03399387172059301
175;0.00016777224494434254;0.032792818282084746
180;0.00018366358547225936;0.03349712821263849
185;0.00016943690650538488;0.032862577742643256
190;0.0001618025940193915;0.03248337118959028
195;0.00017690092029554774;0.03324735426642906
200;0.0001667829201430875;0.03283740021861576
205;0.00016846209161067694;0.032966312840573504
210;0.00017240020081362153;0.03316046604842813
215;0.0001751488597953641;0.03327627342536424
220;0.00018297845340131463;0.03376700108584242
225;0.0001885038498483153;0.033903342370858035
230;0.0001538061150726223;0.03229295968752172
235;0.00015282875710650795;0.032177883609218944
240;0.00016790705644284415;0.03301431589287866
245;0.00017440035520642526;0.033439387596772764
250;0.0001660705075934174;0.03307355291134201
255;0.0001535653416898949;0.03233060791515747
260;0.00018089109449791798;0.03387652283865336
265;0.0001577968292555156;0.03270837173508848
270;0.00016256025213356721;0.032907261571251394
275;0.00016060650875842833;0.032878326254114264
280;0.0001625336190836834;0.032932206748070285
285;0.00015810601799242697;0.032748516786949546
290;0.0001576568496995345;0.0327151710307602
295;0.00017231542858621722;0.03355894983267483
300;0.00015947599599727742;0.032848889128557594
