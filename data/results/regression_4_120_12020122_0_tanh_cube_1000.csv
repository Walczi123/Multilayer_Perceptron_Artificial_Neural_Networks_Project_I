Train dataset: data/regression/data.cube.train.1000.csv
Test dataset: data/regression/data.cube.test.1000.csv
Layers: [1, 64, 32, 16, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.4309991343458229;0.2136262905348122
10;0.44243954315148937;0.21712609000772992
15;0.4297203892203283;0.23778877926599534
20;0.5073912929564058;0.20311114992596568
25;0.48213381919390685;0.21055510967447189
30;0.4028416106216405;0.24164644067382166
35;0.3814252078418212;0.3017591076185228
40;0.4478355975494101;0.21638084562931975
45;0.5318763620187003;0.2361885628838694
50;0.5012060995863731;0.2456670572479716
55;0.46602771513260643;0.1664761172518688
60;0.44702609664701054;0.23174143705961134
65;0.44061185739501973;0.22490412257340042
70;0.44064973177627376;0.23260988142225567
75;0.44925866704253276;0.22136033639306435
80;0.5021343537476824;0.21896172571572756
85;0.43555369439735536;0.2104141448006665
90;0.4044315964616638;0.21668082521104962
95;0.428681588644399;0.22643066299980433
100;0.4457159535420922;0.24089179980374678
105;0.43323635230527907;0.23257620488163497
110;0.3853065090497625;0.34911538340965875
115;0.4073815088004234;0.22080689928945257
120;0.4295112275711188;0.24541612526804876
125;0.4359674090943225;0.2142677439125895
130;0.4787727681209076;0.26372399626172255
135;0.46259694428299797;0.21874098807806217
140;0.5468725750095509;0.2515363034620707
145;0.4292350195388174;0.21867780678163862
150;0.5459248253349572;0.1959511638154386
155;0.4649519298552825;0.199394444296631
160;0.4114916377081333;0.2497123569165135
165;0.4086923035542267;0.2225570766339046
170;0.36896476457285404;0.2894631341946216
175;0.39672630208425763;0.32552412515531964
180;0.444141310897857;0.16917051084160623
185;0.48315438355256346;0.2510137165660005
190;0.42404231184967744;0.25850660516560037
195;0.5147186307209498;0.2186196424601327
200;0.48317425718202683;0.210410284615608
205;0.44603450992632115;0.2468166619317553
210;0.4404986714957562;0.2107964841565854
215;0.46021295518825883;0.23171241010008284
220;0.4414088423937023;0.24180279506133115
225;0.5875105044126713;0.2602004140728847
230;0.3864284602547777;0.35265631005590986
235;0.5012705504478596;0.17047903109442109
240;0.5854280762877146;0.24000427820826017
245;0.42048387392593406;0.2214774940405419
250;0.4566737441813144;0.25071051128588917
255;0.4151395895033736;0.23008514285963794
260;0.44299995040526563;0.21122107320889977
265;0.5388210744833813;0.20289055835197203
270;0.5050874648156768;0.17307517604079106
275;0.47787112597262627;0.22896777468357535
280;0.39979296154412713;0.2593330201217841
285;0.3912107270686053;0.26282331880240833
290;0.43659722922482586;0.24379530477007155
295;0.40511643351857785;0.24180629051424496
300;0.5023430751168919;0.23101249196471488
