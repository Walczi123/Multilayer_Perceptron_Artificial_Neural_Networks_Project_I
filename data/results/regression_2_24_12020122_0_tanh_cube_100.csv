Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 16, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.3842278839121712;0.32828534412928895
10;0.37082147634161594;0.22985530682084987
15;0.40328432610096365;0.21670984713668878
20;0.3660011359884387;0.35768091743898156
25;0.44847296312827273;0.215119044208693
30;0.3741561952182808;0.27175626118894325
35;0.443149943037554;0.19853061664217672
40;0.46924041581797443;0.21213097427110345
45;0.4416185794196104;0.18737376241593875
50;0.3757046500781698;0.24873323465220779
55;0.44388372398789067;0.19814433211788784
60;0.40733406314726583;0.24618349222297894
65;0.515153067915492;0.1884674696342784
70;0.4519114084707795;0.20297182025085067
75;0.4118407840500242;0.19998361007600088
80;0.4892537666719147;0.19650920773250977
85;0.56030995110616;0.15599164720151432
90;0.3722427888307072;0.3305350041080479
95;0.5367595151768902;0.18864337175981355
100;0.4892493883757187;0.19537209113346912
105;0.5932659274083844;0.1867547199036253
110;0.4176611533043801;0.21487790031354118
115;0.4295791996048069;0.20322480211008198
120;0.4489296903405806;0.20847252957748327
125;0.37053901896139163;0.4581629319028276
130;0.4265094123438787;0.21400819254093603
135;0.44943549112159076;0.1954660376817162
140;0.41293947807834797;0.22510071938734333
145;0.3787700474483456;0.22775561583821913
150;0.4068430117061095;0.2289482400164565
155;0.43487974352643016;0.22924114448481903
160;0.4835683535512998;0.2018037632631092
165;0.36203472410041493;0.3536447684435528
170;0.45553253183205916;0.20055054461514296
175;0.41214674398921053;0.2157952542266073
180;0.5950078766245523;0.173397208156989
185;0.5830748497950718;0.19069425806763493
190;0.4418689401822208;0.18169512302477825
195;0.3989549777006514;0.20356242514526562
200;0.43473441299739685;0.2027610973715386
205;0.4914584863863675;0.1518937857243721
210;0.5042591337330048;0.2002629437616709
215;0.45856421988818863;0.20161081807667353
220;0.5671553409314859;0.19299341562042158
225;0.4174877813795339;0.2003581137838569
230;0.4311485609888163;0.20448364180706205
235;0.36373744759306736;0.3730505555253505
240;0.362944803732864;0.32219269501284986
245;0.4240586688117514;0.17268041663334663
250;0.4820638870696253;0.18139395016970722
255;0.40579497857462843;0.21780604713336024
260;0.4927577004266653;0.19299912130263064
265;0.37052323556498407;0.28505046836918413
270;0.43068793418447143;0.18955284884780726
275;0.38737116300883345;0.20274418917578285
280;0.4099579962751384;0.23189403650176593
285;0.4148671307674059;0.20596186545985448
290;0.45833181268308315;0.21011953075690984
295;0.42365967751730327;0.19536338674879314
300;0.47523910804894026;0.20680449153152508
