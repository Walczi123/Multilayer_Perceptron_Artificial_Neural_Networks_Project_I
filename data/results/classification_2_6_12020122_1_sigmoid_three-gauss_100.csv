Train dataset: data/classification/data.three_gauss.train.100.csv
Test dataset: data/classification/data.three_gauss.test.100.csv
Layers: [2, 4, 2, 3]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: sigmoid
Output function: softmax
Problem type: classification
Epochs: 300
Epochs;Loss (cross_entropy);Loss (hinge);Accuracy
5;-526.985773390937;0.7533333333333333;62.33333333333333
10;-424.83047672722165;0.12666666666666668;92.33333333333333
15;-437.3206762606791;0.10666666666666667;92.66666666666666
20;-440.4987298743603;0.12;92.33333333333333
25;-444.1479156973338;0.1;92.66666666666666
30;-444.1479156973338;0.1;92.66666666666666
35;-444.619047906626;0.08666666666666667;92.66666666666666
40;-443.3505366978291;0.08666666666666667;92.33333333333333
45;-436.11783216973294;0.07333333333333333;92.66666666666666
50;-464.2241689158567;0.08666666666666667;92.66666666666666
55;-466.65695946450575;0.08;92.66666666666666
60;-464.2241689158567;0.08666666666666667;92.66666666666666
65;-440.74784716238474;0.06666666666666667;93.33333333333333
70;-465.32278113785816;0.08666666666666667;92.33333333333333
75;-444.7368309589491;0.07333333333333333;93.33333333333333
80;-466.65695946450575;0.08;92.66666666666666
85;-458.03796067528947;0.08;92.0
90;-462.48452549776766;0.07333333333333333;92.66666666666666
95;-460.1174020669693;0.07333333333333333;92.66666666666666
100;-438.7205217051773;0.06666666666666667;93.33333333333333
105;-461.7913783672077;0.06666666666666667;93.0
110;-467.7555716865072;0.08;92.33333333333333
115;-461.7913783672077;0.06666666666666667;93.0
120;-461.7913783672077;0.06666666666666667;93.0
125;-438.7205217051773;0.06666666666666667;93.33333333333333
130;-447.3916364288658;0.06;93.33333333333333
135;-462.6023085500907;0.08;92.66666666666666
140;-442.17270617459866;0.14;92.33333333333333
145;-465.72824622929966;0.08;92.33333333333333
150;-447.96700050710274;0.08;92.33333333333333
155;-461.7913783672077;0.06666666666666667;93.0
160;-447.50941948118884;0.06666666666666667;93.33333333333333
165;-461.7913783672077;0.06666666666666667;93.0
170;-471.33909039163007;0.06666666666666667;92.66666666666666
175;-464.91731604641666;0.06666666666666667;92.66666666666666
180;-455.77506911343585;0.06666666666666667;93.33333333333333
185;-462.88999058920916;0.06666666666666667;92.66666666666666
190;-462.6023085500907;0.08;92.66666666666666
195;-465.72824622929966;0.08;92.33333333333333
200;-461.7913783672077;0.06666666666666667;93.0
205;-461.7913783672077;0.06666666666666667;93.0
210;-464.6296340072982;0.08;92.66666666666666
215;-464.6296340072982;0.08;92.66666666666666
220;-462.6023085500907;0.08;92.66666666666666
225;-461.7913783672077;0.06666666666666667;93.0
230;-461.7913783672077;0.06666666666666667;93.0
235;-461.7913783672077;0.06666666666666667;93.0
240;-451.73396938239927;0.06666666666666667;92.66666666666666
245;-448.2025666117488;0.07333333333333333;93.0
250;-462.88999058920916;0.06666666666666667;92.66666666666666
255;-456.92579726990976;0.10666666666666667;92.66666666666666
260;-462.6023085500907;0.08;92.66666666666666
265;-462.88999058920916;0.06666666666666667;92.66666666666666
270;-458.02440949191123;0.10666666666666667;92.33333333333333
275;-460.6927661452063;0.06666666666666667;93.33333333333333
280;-456.12841827040506;0.06666666666666667;92.66666666666666
285;-468.1610367779487;0.05333333333333334;93.0
290;-454.1010928131975;0.06666666666666667;92.66666666666666
295;-462.6023085500907;0.08;92.66666666666666
300;-462.6023085500907;0.08;92.66666666666666
