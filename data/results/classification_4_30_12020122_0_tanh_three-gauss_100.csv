Train dataset: data/classification/data.three_gauss.train.100.csv
Test dataset: data/classification/data.three_gauss.test.100.csv
Layers: [2, 16, 8, 4, 2, 3]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: softmax
Problem type: classification
Epochs: 300
Epochs;Loss (cross_entropy);Loss (hinge);Accuracy
5;-407.82804525343533;0.14;91.0
10;-409.9074866451152;0.15333333333333332;91.0
15;-411.29378090623516;0.16666666666666666;90.33333333333333
20;-407.3047971096708;0.16;90.33333333333333
25;-406.7815489659063;0.18;89.66666666666666
30;-412.6800751673551;0.16;90.66666666666666
35;-409.2278906979336;0.08;92.0
40;-412.51017618055965;0.15333333333333332;90.66666666666666
45;-413.8964704416795;0.14666666666666667;91.0
50;-416.10724606906075;0.08;91.33333333333333
55;-412.4716114294656;0.04;92.66666666666666
60;-412.4716114294656;0.04;92.66666666666666
65;-410.26083580208433;0.11333333333333333;91.66666666666666
70;-410.7840839458489;0.1;91.33333333333333
75;-413.72657145488415;0.14;91.0
80;-413.72657145488415;0.12666666666666668;91.0
85;-413.452440599144;0.04;93.0
90;-417.0224081208885;0.12666666666666668;91.33333333333333
95;-419.62509765633285;0.12;92.0
100;-416.1593620035331;0.11333333333333333;91.66666666666666
105;-413.7265714548841;0.12666666666666668;91.0
110;-416.3292609903285;0.12;91.66666666666666
115;-406.3239679399925;0.14666666666666667;90.0
120;-413.72657145488415;0.14;91.0
125;-415.70178097761925;0.08;92.0
130;-422.73748415216346;0.18666666666666668;90.66666666666666
135;-426.6222360797831;0.12;91.66666666666666
140;-427.236164909114;0.2733333333333333;87.66666666666667
145;-412.1047110891181;0.14666666666666667;89.33333333333333
150;-422.39768617857266;0.14;91.66666666666666
155;-412.27461007591353;0.16666666666666666;89.33333333333333
160;-417.81978712039313;0.2;89.66666666666666
165;-411.46367989303053;0.18;89.33333333333333
170;-413.3867734812933;0.09333333333333334;92.0
175;-412.3402771937643;0.11333333333333333;90.33333333333333
180;-401.7460688818129;0.20666666666666667;89.33333333333333
185;-393.40120094833685;0.32;87.0
190;-415.6361138597686;0.11333333333333333;92.0
195;-413.26899042897026;0.10666666666666667;91.33333333333333
200;-404.87200656102186;0.18666666666666668;89.66666666666666
205;-405.6172696260541;0.21333333333333335;88.66666666666667
210;-415.11286571600397;0.13333333333333333;91.33333333333333
215;-415.7017809776193;0.09333333333333334;92.0
220;-413.7922385727348;0.1;92.0
225;-408.5211923839953;0.13333333333333333;90.66666666666666
230;-423.260732295928;0.16666666666666666;91.33333333333333
235;-430.0879717325828;0.16;91.33333333333333
240;-428.8715764582583;0.15333333333333332;91.66666666666666
245;-413.0334243243242;0.12;91.33333333333333
250;-412.562292115032;0.17333333333333334;90.0
255;-415.5704467419179;0.16666666666666666;88.33333333333333
260;-404.98978961334495;0.18;88.33333333333333
265;-410.26083580208444;0.11333333333333333;91.66666666666666
270;-414.2498195986486;0.10666666666666667;91.66666666666666
275;-419.97844681330207;0.1;92.0
280;-420.1483458000974;0.1;92.66666666666666
285;-420.8414929306573;0.10666666666666667;92.33333333333333
290;-413.0334243243242;0.12;91.33333333333333
295;-412.0004792201734;0.08;92.66666666666666
300;-418.4087023820083;0.13333333333333333;91.66666666666666
