Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 16, 8, 1]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.00048119806216662657;0.020550532891562393
10;0.0003445025365143056;0.01920405101803328
15;0.00030970780697468645;0.01912669121813471
20;0.00028144264009397557;0.019149184938351507
25;0.00021253576746484086;0.018090050365628475
30;0.0002564373880112408;0.019314645023324698
35;0.0002449291805096458;0.019442283097283804
40;0.0002645055343311396;0.020073464304475323
45;0.0003245796836188528;0.021504714766814346
50;0.00032908130846836153;0.021761060347805568
55;0.00033061229181554296;0.021807781737865475
60;0.0003675168904195907;0.022584677940704206
65;0.0003817683776302689;0.02285895731539783
70;0.00041370504833715806;0.023483150458360594
75;0.00043527692363609503;0.023860222619250665
80;0.00045380682283862104;0.02419657245252177
85;0.0004788581951575297;0.0246419763318173
90;0.00047994953007770446;0.024669306529837298
95;0.0004795664630067298;0.024675674109983524
100;0.0004978929943442176;0.024980232738845195
105;0.0005195511509706877;0.02533273154494573
110;0.000554471597882531;0.02588459502161199
115;0.00054543417666104;0.02574552159541029
120;0.0005696431825585234;0.02611481879555618
125;0.0005641220479828544;0.026035084468734596
130;0.000565142698157616;0.026056152845549893
135;0.0005881345289968112;0.02639348343959562
140;0.0005922267381349784;0.02647203356468401
145;0.0005884424180218815;0.02640766868755014
150;0.0006081654087389083;0.02670164637250177
155;0.0005984371200030137;0.02657196972921314
160;0.0006070920824644334;0.0266952930053485
165;0.0006195823076397428;0.026874428201982718
170;0.0006151366150929138;0.026824888169469364
175;0.000618940302796514;0.026872975059910438
180;0.0006156534805722984;0.026832611978575206
185;0.0006320941059342513;0.027070885722934115
190;0.0006254637618445332;0.02698511659407232
195;0.0006168131209081631;0.026872044704378248
200;0.0006389220723395208;0.027184277693939152
205;0.0006333799819931348;0.02711647538324545
210;0.0006249763531784511;0.026996837361398768
215;0.000633026444956486;0.027113627164543627
220;0.0006283940187852112;0.027057176070160442
225;0.0006254013173514932;0.027027363513370328
230;0.0006284560451636493;0.027078478135768386
235;0.0006370803922803554;0.027194411096965826
240;0.0006310960803717172;0.02712070283083437
245;0.0006322177217587833;0.02714079960829754
250;0.0006346268498861336;0.02717586755791738
255;0.00061080633342231;0.02685400703948439
260;0.0006112318497220005;0.02686532301470165
265;0.0006324362837026994;0.0271671918258092
270;0.0006319165199070476;0.027173803203806635
275;0.0006232973890277398;0.027053224248954205
280;0.000623510640271147;0.027056390416355203
285;0.0006251827278007621;0.02708408844220094
290;0.0006136619986719374;0.02692836751924164
295;0.0006186196156190191;0.02700969683983903
300;0.0006227632135350613;0.027066210498844133
