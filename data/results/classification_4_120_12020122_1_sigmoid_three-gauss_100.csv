Train dataset: data/classification/data.three_gauss.train.100.csv
Test dataset: data/classification/data.three_gauss.test.100.csv
Layers: [2, 64, 32, 16, 8, 3]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: sigmoid
Output function: softmax
Problem type: classification
Epochs: 300
Epochs;Loss (cross_entropy);Loss (hinge);Accuracy
5;-659.1673932008655;1.3333333333333333;33.33333333333333
10;-417.87190305486547;0.24;89.33333333333333
15;-439.4657847702096;0.06666666666666667;93.66666666666667
20;-428.7673445893136;0.09333333333333334;92.66666666666666
25;-462.7065404190355;0.11333333333333333;91.66666666666666
30;-449.405410702695;0.10666666666666667;93.0
35;-431.0437873345455;0.08;92.0
40;-427.61661643283975;0.07333333333333333;92.66666666666666
45;-426.45233709298765;0.10666666666666667;92.66666666666666
50;-464.6296340072982;0.08;92.66666666666666
55;-452.20510159169146;0.08;92.33333333333333
60;-442.6573895672692;0.08;92.66666666666666
65;-430.7946700465211;0.06666666666666667;93.0
70;-462.72009160241373;0.06666666666666667;93.33333333333333
75;-438.3150566137358;0.07333333333333333;93.33333333333333
80;-471.5089893784254;0.06666666666666667;92.0
85;-469.024082895304;0.08;91.33333333333333
90;-461.15034717112013;0.08;92.33333333333333
95;-449.8244269775148;0.05333333333333334;93.33333333333333
100;-436.2877311565283;0.07333333333333333;93.33333333333333
105;-449.30117883375027;0.07333333333333333;92.66666666666666
110;-452.02165142151773;0.05333333333333334;92.66666666666666
115;-446.52859031151047;0.05333333333333334;93.0
120;-454.5586738391114;0.08;93.0
125;-444.0957997628615;0.06;93.0
130;-446.9861713374243;0.06666666666666667;93.33333333333333
135;-452.88469753887307;0.06;93.0
140;-460.1695180014417;0.06666666666666667;93.33333333333333
145;-448.2025666117488;0.07333333333333333;93.0
150;-451.60263514669793;0.10666666666666667;92.33333333333333
155;-459.7119369755278;0.05333333333333334;93.0
160;-442.9836363574817;0.14666666666666667;91.66666666666666
165;-467.6899045686565;0.04;93.33333333333333
170;-461.67359531488466;0.06;93.0
175;-459.82972002785084;0.04;93.66666666666667
180;-458.0900766097618;0.07333333333333333;92.66666666666666
185;-450.92303919951627;0.05333333333333334;93.0
190;-458.4955417012033;0.04666666666666667;93.33333333333333
195;-460.9283322498523;0.04;93.33333333333333
200;-458.0900766097618;0.07333333333333333;92.66666666666666
205;-455.65728606111287;0.08;92.66666666666666
210;-439.81913392717877;0.06666666666666667;93.0
215;-430.62477105972573;0.07333333333333333;93.0
220;-456.1805342048774;0.06;93.33333333333333
225;-457.97229355743883;0.08666666666666667;92.0
230;-458.78322374032183;0.05333333333333334;94.0
235;-450.5696900425471;0.05333333333333334;93.66666666666667
240;-450.11210901663327;0.04;93.33333333333333
245;-446.0574581022183;0.06666666666666667;93.0
250;-460.5228671584108;0.04666666666666667;93.33333333333333
255;-459.1230217139127;0.08;92.33333333333333
260;-447.22173744207043;0.06;92.66666666666666
265;-457.2791464268788;0.06;93.0
270;-463.6352536542415;0.06666666666666667;93.0
275;-449.70664392519177;0.04666666666666667;93.33333333333333
280;-441.6630092142125;0.06666666666666667;93.0
285;-465.32278113785816;0.04;93.33333333333333
290;-457.10924744008344;0.06;93.66666666666667
295;-452.88469753887307;0.06;93.0
300;-455.25182096967137;0.06;93.0
