Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.36676844182347224;0.09790703373480826
10;0.2771783299356922;0.09601419752292893
15;0.3291430423483766;0.1006718719901874
20;0.24264586966614948;0.11751805142965534
25;0.40821266161096253;0.09825746497394217
30;0.3011093469915209;0.09387830015830975
35;0.2830782247091804;0.10040954697145092
40;0.257989613112991;0.10184774338838877
45;0.3832981928540871;0.10799643049679664
50;0.33020909580448715;0.10536495556871796
55;0.354461049497763;0.10273900455024787
60;0.2741224528965928;0.10275493506228123
65;0.29509208919614033;0.10342497923174718
70;0.34628249020123314;0.10068762738330318
75;0.33520801590358545;0.10523611208507803
80;0.293283388365212;0.1037415180533544
85;0.29181265414540475;0.09566001334756291
90;0.2840805730004178;0.10224119766435695
95;0.33940559949353455;0.1079234618300284
100;0.34851601733605325;0.09778430568327216
105;0.5445259357153565;0.11042691495224695
110;0.2908727674298689;0.10058913425909335
115;0.2724058239684224;0.10304565923570808
120;0.33838932983659414;0.09979037003705922
125;0.3258810318305952;0.0989777401510138
130;0.4123936501398513;0.12436922636033851
135;0.36563041383501343;0.09789079705986181
140;0.3879878649267542;0.10875409118613652
145;0.43157705390575896;0.10757902710440088
150;0.2672001328976679;0.11262974856633493
155;0.2527877944450969;0.09718519689132497
160;0.533464573571256;0.1082884563145079
165;0.32220269647432354;0.09666565527739465
170;0.2373794011066077;0.11055747742524442
175;0.25652232838270006;0.09856383873548644
180;0.4205768189025114;0.10832593133336264
185;0.2592883954640654;0.10479872866633554
190;0.24190180977069653;0.09513165187382105
195;0.31100563623343497;0.10374833936815685
200;0.38189774835374546;0.10435230438653893
205;0.3579460506813285;0.10363828459411491
210;0.3454210822735452;0.10410930153835601
215;0.34098587037178485;0.09808012662032761
220;0.3139117951576987;0.10722518495788741
225;0.2530985662708416;0.11156150725525443
230;0.28537560066490236;0.10693947620838548
235;0.3649382469297914;0.10648477013088196
240;0.315619170947598;0.09730539274683389
245;0.452081557855971;0.11135421854084744
250;0.2406660149237502;0.09921238594245245
255;0.23283661811154607;0.10226235942406552
260;0.23280531285001796;0.11295183274154792
265;0.33740441380194086;0.104492097672881
270;0.31318823429425013;0.10278889728912866
275;0.35538856108079386;0.10141418504104462
280;0.41903519362503855;0.1110926475221686
285;0.33134844606732683;0.1091830810992915
290;0.3024910443551163;0.09948245002971282
295;0.27381313415286046;0.09602430510392267
300;0.2725843673911883;0.10330231714741389
