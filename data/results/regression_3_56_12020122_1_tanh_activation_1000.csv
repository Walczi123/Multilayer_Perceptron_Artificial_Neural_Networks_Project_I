Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 32, 16, 8, 1]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.00015024568282134775;0.024647385637818903
10;0.00010527617468555577;0.02306003883879018
15;8.504579420398228e-05;0.02182316380760347
20;7.789216008365066e-05;0.021580809866898174
25;6.495389624231926e-05;0.020921690592956083
30;7.075728966031907e-05;0.02153566392784541
35;6.367085065023201e-05;0.021096915163059934
40;6.542082584260938e-05;0.0213323461994292
45;6.654523060106742e-05;0.021213410399058958
50;5.863619700500941e-05;0.02070171220272011
55;6.014169235508385e-05;0.02083849365548167
60;5.8204207013311645e-05;0.020787690221066215
65;6.113907589522031e-05;0.02119815058562907
70;5.988872964220788e-05;0.02109813536459234
75;6.143763049893785e-05;0.021330436479673025
80;6.30694757237707e-05;0.021432770480573033
85;6.18470492137923e-05;0.0213486583892993
90;5.7009803962311915e-05;0.021011956886627808
95;5.900894994181841e-05;0.021269474822033462
100;5.993097786058984e-05;0.021337470430293534
105;6.172141856408441e-05;0.021567623574671273
110;5.965295686308579e-05;0.02137256627979719
115;5.666005628430202e-05;0.021133164769346423
120;5.9773774325259895e-05;0.02140561871843497
125;6.303383767100713e-05;0.021804287449338697
130;6.045809996319996e-05;0.02155482560858519
135;5.9096228905163567e-05;0.021464106467956087
140;6.503087540325857e-05;0.021931802566307237
145;6.229897833468146e-05;0.02176385625380482
150;6.400381589521886e-05;0.021908436942119146
155;6.359618657983844e-05;0.021981362954492996
160;6.477169242153269e-05;0.022047138292754214
165;6.273007814664125e-05;0.021875956051554072
170;6.343354229532833e-05;0.021965037625529208
175;6.24487823578346e-05;0.021883061222476585
180;6.492410733789987e-05;0.022137214519199193
185;6.477398053413991e-05;0.022129363362255416
190;6.588993238616333e-05;0.022268150798651843
195;6.636746448306331e-05;0.02230590750463231
200;6.437811829317666e-05;0.02212141584971874
205;6.847738163943207e-05;0.02247238967824654
210;6.552712662018088e-05;0.022268522626775606
215;6.807524770174783e-05;0.022501310617964437
220;7.030055470225121e-05;0.022696988670226372
225;6.837570835630552e-05;0.022489370261198918
230;7.041678140962547e-05;0.022668602585285715
235;6.833259661177987e-05;0.02251108318809153
240;7.028095352916787e-05;0.022727214595544724
245;7.099004317660106e-05;0.022793539622313647
250;6.904338705463416e-05;0.022615104053064192
255;7.192259185827647e-05;0.022850444889171127
260;7.117188330718689e-05;0.022821939742875545
265;6.931495610458852e-05;0.022653650046355847
270;7.050595469922703e-05;0.022767672231977433
275;7.369320917349633e-05;0.023052533641781992
280;7.105698754866675e-05;0.022850058743707925
285;7.198660300643887e-05;0.022939949963123568
290;7.097678484555399e-05;0.022823593923687834
295;7.425657993677028e-05;0.02311542423545951
300;7.41293208663178e-05;0.023128653685017472
