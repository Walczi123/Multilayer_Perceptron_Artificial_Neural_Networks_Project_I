Train dataset: data/classification/data.three_gauss.train.100.csv
Test dataset: data/classification/data.three_gauss.test.100.csv
Layers: [2, 8, 4, 2, 3]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: softmax
Problem type: classification
Epochs: 300
Epochs;Loss (cross_entropy);Loss (hinge);Accuracy
5;-425.3537248709862;0.09333333333333334;93.0
10;-434.6658707907623;0.07333333333333333;93.66666666666667
15;-446.1095740366907;0.10666666666666667;92.66666666666666
20;-435.0713358822038;0.06666666666666667;93.66666666666667
25;-431.0166849677888;0.11333333333333333;92.66666666666666
30;-439.8712498616511;0.06;93.66666666666667
35;-431.6056002294041;0.06;93.33333333333333
40;-433.6850416210839;0.07333333333333333;93.33333333333333
45;-432.46864634675944;0.08666666666666667;93.0
50;-457.514712531525;0.05333333333333334;93.66666666666667
55;-456.12841827040506;0.06666666666666667;92.66666666666666
60;-426.6357872631614;0.05333333333333334;93.0
65;-422.52902041427404;0.04666666666666667;93.66666666666667
70;-460.81054919752927;0.05333333333333334;92.66666666666666
75;-432.75632838587785;0.08;93.66666666666667
80;-467.87335473883024;0.06666666666666667;93.0
85;-460.9283322498523;0.04;93.33333333333333
90;-459.5941539232048;0.06666666666666667;92.33333333333333
95;-458.6133247535264;0.05333333333333334;93.33333333333333
100;-451.3806202254301;0.08666666666666667;92.66666666666666
105;-449.30117883375027;0.07333333333333333;92.66666666666666
110;-471.5089893784254;0.06666666666666667;92.0
115;-460.5228671584108;0.06666666666666667;92.66666666666666
120;-456.1805342048774;0.06;93.33333333333333
125;-451.6683022645485;0.05333333333333334;93.33333333333333
130;-449.06561272910415;0.08;93.33333333333333
135;-441.8985753188586;0.05333333333333334;93.0
140;-440.6821800445341;0.05333333333333334;94.0
145;-468.21315271242105;0.06666666666666667;93.0
150;-444.8410628278938;0.08;92.66666666666666
155;-457.51471253152494;0.05333333333333334;93.66666666666667
160;-451.6683022645486;0.07333333333333333;94.0
165;-469.0761988297764;0.07333333333333333;92.0
170;-424.55634587148154;0.04;93.0
175;-474.05956297939747;0.09333333333333334;91.66666666666666
180;-459.1886888317633;0.07333333333333333;92.33333333333333
185;-460.40508410608777;0.06;92.66666666666666
190;-466.0680442028905;0.06;93.0
195;-464.73386587624293;0.06666666666666667;92.66666666666666
200;-455.3696040219944;0.09333333333333334;92.66666666666666
205;-449.30117883375027;0.07333333333333333;92.66666666666666
210;-460.40508410608777;0.06;92.66666666666666
215;-447.2738533765428;0.09333333333333334;93.0
220;-448.4381327163949;0.04666666666666667;93.0
225;-459.1886888317633;0.07333333333333333;92.33333333333333
230;-464.10638586353366;0.05333333333333334;93.0
235;-435.71236707829144;0.04666666666666667;93.66666666666667
240;-448.3203496640719;0.05333333333333334;93.0
245;-442.9315204230093;0.09333333333333334;93.0
250;-447.7971015203073;0.08;93.0
255;-457.21347930902823;0.09333333333333334;92.66666666666666
260;-450.28200800342864;0.08666666666666667;91.66666666666666
265;-453.695627721756;0.07333333333333333;94.0
270;-457.50116134814664;0.1;92.0
275;-456.9914643877604;0.07333333333333333;93.0
280;-439.6492349403833;0.04666666666666667;93.0
285;-466.3036103075366;0.05333333333333334;93.66666666666667
290;-456.9914643877604;0.07333333333333333;93.0
295;-442.9450716063877;0.04666666666666667;93.33333333333333
300;-456.4161003095235;0.04666666666666667;93.33333333333333
