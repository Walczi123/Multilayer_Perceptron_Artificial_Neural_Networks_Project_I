Train dataset: data/classification/data.three_gauss.train.100.csv
Test dataset: data/classification/data.three_gauss.test.100.csv
Layers: [2, 128, 64, 32, 3]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: softmax
Problem type: classification
Epochs: 300
Epochs;Loss (cross_entropy);Loss (hinge);Accuracy
5;-437.9095915222943;0.05333333333333334;93.66666666666667
10;-438.83830475750034;0.05333333333333334;94.0
15;-456.1805342048774;0.06;93.33333333333333
20;-445.88755911542285;0.06666666666666667;93.66666666666667
25;-466.3036103075366;0.05333333333333334;92.33333333333333
30;-440.0932647829188;0.12666666666666668;92.33333333333333
35;-443.99156789391674;0.05333333333333334;93.66666666666667
40;-442.99718754086007;0.05333333333333334;92.66666666666666
45;-430.1536388504335;0.08666666666666667;93.0
50;-465.38844825570885;0.08;92.33333333333333
55;-465.8460292816227;0.06666666666666667;93.0
60;-436.52329726117443;0.06666666666666667;92.66666666666666
65;-438.8904206919727;0.04666666666666667;93.33333333333333
70;-459.7119369755278;0.05333333333333334;93.0
75;-433.8549406078793;0.08;93.33333333333333
80;-461.56936344594;0.05333333333333334;93.66666666666667
85;-448.15045067727647;0.08;92.33333333333333
90;-460.81054919752927;0.05333333333333334;92.66666666666666
95;-453.1723795779915;0.04666666666666667;93.0
100;-437.7261413521205;0.1;92.66666666666666
105;-441.6630092142125;0.06666666666666667;93.0
110;-447.96700050710274;0.08;92.33333333333333
115;-448.15045067727647;0.08;92.33333333333333
120;-456.92579726990976;0.08;93.0
125;-449.3532947682226;0.06666666666666667;93.33333333333333
130;-438.5235203516253;0.18;91.33333333333333
135;-449.8108757941365;0.1;93.0
140;-448.35891441516594;0.14666666666666667;91.66666666666666
145;-462.380293628823;0.06666666666666667;93.33333333333333
150;-440.4987298743603;0.12;92.33333333333333
155;-457.39692947920184;0.06666666666666667;93.0
160;-446.1752411545413;0.05333333333333334;93.66666666666667
165;-471.5089893784254;0.06666666666666667;92.0
170;-462.88999058920916;0.06666666666666667;92.66666666666666
175;-460.2351851192924;0.08;92.66666666666666
180;-457.8023945706434;0.04;93.66666666666667
185;-477.6566328678985;0.17333333333333334;89.33333333333333
190;-449.2355117158995;0.08;92.66666666666666
195;-466.13371132074116;0.05333333333333334;93.0
200;-452.53134838190385;0.08;93.0
205;-445.2986438538076;0.11333333333333333;92.33333333333333
210;-462.88999058920916;0.06666666666666667;92.66666666666666
215;-466.0159282684181;0.06666666666666667;92.33333333333333
220;-458.6133247535264;0.05333333333333334;93.33333333333333
225;-456.29831725720044;0.06666666666666667;93.33333333333333
230;-452.649131434227;0.08666666666666667;93.0
235;-451.7860853168716;0.06;93.33333333333333
240;-454.9120229960805;0.06;93.0
245;-453.2901626303145;0.05333333333333334;93.0
250;-452.88469753887307;0.06;93.0
255;-445.9917909843675;0.12;92.0
260;-448.4902486508673;0.06;93.0
265;-456.1805342048774;0.06;93.33333333333333
270;-459.35858781855876;0.07333333333333333;93.0
275;-460.4572000405602;0.07333333333333333;92.66666666666666
280;-461.09823123664773;0.04;92.66666666666666
285;-463.53102178529673;0.08;91.66666666666666
290;-455.31748808752207;0.05333333333333334;93.0
295;-441.72867633206323;0.04666666666666667;93.0
300;-459.8818359623233;0.03333333333333333;94.33333333333334
