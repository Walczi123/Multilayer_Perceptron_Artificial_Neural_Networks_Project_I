Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 256, 128, 64, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.380642358367145;0.16785223892078813
10;0.2480116614979218;0.12043093117962231
15;0.3537193503640005;0.1392610315888844
20;0.2855275882377375;0.13272640267461078
25;0.3493286219974414;0.14838417918273
30;0.3161110017611341;0.13992220072879155
35;0.37100626347567994;0.16165554314755032
40;0.24787395074655252;0.1390396054986775
45;0.3462155232692425;0.11937234946511635
50;0.2833668663037467;0.15291721216468152
55;0.31684432592167133;0.12807731448723159
60;0.3559591315653435;0.14387368954001825
65;0.3276626673456884;0.14683994787268112
70;0.29792073046107953;0.12354148210387927
75;0.3063179912791488;0.12564415223037195
80;0.314080973663476;0.11900800307571663
85;0.3676652397701705;0.15435096141560953
90;0.34552294880237916;0.13687787377066074
95;0.3229638185750777;0.11852609846370732
100;0.3293348001299921;0.14218322832103475
105;0.3405893821859425;0.11335898864892441
110;0.294750242084208;0.1306798021521662
115;0.36904953654345574;0.14326526726262273
120;0.29711144365011627;0.12017531133693943
125;0.31224431886937476;0.14000826266799077
130;0.3638254839121149;0.1270044308183082
135;0.28869802282230705;0.13883935782566317
140;0.3357148919370637;0.1393884641510227
145;0.37967452008703245;0.1613484048217988
150;0.34213003951216636;0.13465669125134844
155;0.30089739929207066;0.12462947225273058
160;0.3155928718701277;0.14043755312577
165;0.3633219699180615;0.13034090992440614
170;0.32676979767323233;0.13724055437741253
175;0.3269054339946106;0.13198824559292768
180;0.31150997634142785;0.13648399546086773
185;0.31122344498040533;0.12432058006103407
190;0.314426315744692;0.12813398167858675
195;0.35595205588772216;0.13909432003043776
200;0.3224887460554752;0.13821859114328158
205;0.3594962360682581;0.1586752231822488
210;0.3204302927701799;0.12811555394703483
215;0.33184505011071574;0.13398762235552128
220;0.32474605084182234;0.13415323231353227
225;0.31009266065899693;0.128270594953931
230;0.34356035933368145;0.1375948612828767
235;0.3754229514348806;0.13884241792859853
240;0.36358719563765596;0.15547059993223747
245;0.31420199384427994;0.1287400714931564
250;0.3510741550660739;0.14527785599419954
255;0.27256664740786396;0.13420326538071775
260;0.3168950415579517;0.13429013432587228
265;0.32983912457934755;0.1392725539813995
270;0.2658725496341747;0.13368731617082247
275;0.34575274007776674;0.1432255131505431
280;0.32924966408429257;0.14220518555361936
285;0.31444428881899283;0.1364705390471152
290;0.31285931100116837;0.12479478290591706
295;0.30798852024802015;0.1391410050400421
300;0.33331210295553804;0.13185194110579962
