Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 8, 4, 2, 1]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.007622654864776016;inf
10;0.0017076696613905849;inf
15;0.0013391639726097007;inf
20;0.00021459428087180522;0.016376295818585126
25;0.00015928008310848885;0.01789660127752594
30;0.0002813961863826069;0.024481738868240684
35;0.00041305951012095396;0.027436406930158596
40;0.000604827377699129;0.03052501239423896
45;0.0006858445607921671;0.03141905543714742
50;0.000784307715837169;0.03265626512655968
55;0.0008324261759784284;0.033145661760317836
60;0.0008434054441359233;0.03324114218339595
65;0.0008828239137284151;0.03366423891778494
70;0.0008877276499716218;0.03370439397284994
75;0.0008759050800495972;0.03358397328169383
80;0.0008686660251814483;0.03349207001581103
85;0.0008799137834093789;0.03363667421237109
90;0.000849028293295612;0.033294117813764595
95;0.0008746788983195614;0.03357175032858237
100;0.000878582298678171;0.0336472701324443
105;0.0008903500151520848;0.03376695546378842
110;0.0008372248506404116;0.03320411760691443
115;0.0008581546698060281;0.03343797295865365
120;0.0008510325992495046;0.033369742329612075
125;0.000868099938255614;0.03356813610854776
130;0.0008432832880715175;0.03328163723461627
135;0.0008816489323527254;0.033734908050426005
140;0.0008602734093487789;0.03348208856605361
145;0.0008486361489026697;0.0333844174818883
150;0.0008534213476537439;0.03344745787329828
155;0.0008458550364833755;0.03337872829807054
160;0.0008578652184401889;0.033522981114930184
165;0.000848863861212863;0.033443867491549185
170;0.0008607447660901539;0.03359732187686009
175;0.0008421078749487717;0.033367436692434686
180;0.0008317956923393792;0.03327221391706071
185;0.0008493180029061092;0.03348234357067523
190;0.0008496616668555145;0.03347695732170616
195;0.0008622030636073708;0.03360881126362543
200;0.0008567960414535381;0.03357763299999646
205;0.0008536408776903955;0.03356146896443854
210;0.0008263514973878671;0.033241614148230375
215;0.0008525334551012019;0.0335814523850866
220;0.000844008973302233;0.0334667647216592
225;0.0008534157658315512;0.03360178044852704
230;0.0008148227040686805;0.03314742606307682
235;0.0008328884572525769;0.03338552676483001
240;0.000822306978093393;0.0332517265580557
245;0.0008465728756764637;0.033546985593502485
250;0.0008527374198946319;0.03360827058321648
255;0.0008165115054539974;0.03320079527897174
260;0.0008410341330269544;0.03352249159898897
265;0.0008390335884922308;0.03348017204045087
270;0.0008029188664712365;0.03306021821358881
275;0.0008161870484574055;0.03325152124117589
280;0.000833517129415526;0.03345525381519084
285;0.0008144583176032452;0.03322149384877123
290;0.0008162295336984275;0.03324804609476978
295;0.0008483756808716968;0.03365774834990298
300;0.0008178190027906548;0.03329407565274038
