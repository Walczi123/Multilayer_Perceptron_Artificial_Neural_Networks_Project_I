Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 64, 32, 16, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.3841697284588528;0.5743554949786946
10;0.36966482967823705;0.2893976460798539
15;0.4020585646769059;0.24661943320983393
20;0.37089154619081627;0.429128042018099
25;0.4422140732824678;0.2301620950661186
30;0.3726352111128191;0.3096668183398877
35;0.4287418410596307;0.21841062047168613
40;0.4637601983080878;0.22786434442264972
45;0.43677811586470977;0.20620056698781733
50;0.3732151934581848;0.3013019421095396
55;0.41803702364545614;0.22794640521260837
60;0.4128608590689396;0.2520671242274123
65;0.4981299845981652;0.2029890459642753
70;0.4635743754693624;0.20021710315839664
75;0.40605573065553213;0.2235475490924615
80;0.49071354149035695;0.2069752424018209
85;0.5561325132790219;0.17236169879483385
90;0.37901556161373157;0.31331474088742783
95;0.5411642825205326;0.19778002765767597
100;0.46703217307725664;0.21232072134029625
105;0.5857918497074939;0.200719566991767
110;0.41812864632235486;0.22378677949414838
115;0.4076430881349598;0.23018212819404846
120;0.4463376969887254;0.21425960747554978
125;0.37024386374932017;0.32369958244855856
130;0.4163163463349394;0.22941312031861755
135;0.4505730730039809;0.2033544088375631
140;0.383624282432248;0.2907622533886814
145;0.3736801552323228;0.2516907004232253
150;0.40779204192732366;0.2371852237387979
155;0.4131607189230798;0.24652336579070191
160;0.48984983242604657;0.21185280460480604
165;0.36345794328508924;0.2864915450355662
170;0.4344313249439715;0.211390205115092
175;0.40758210216998064;0.22687015923189885
180;0.5128733153283821;0.1941704579966579
185;0.5413598715244277;0.20616162220150389
190;0.438986686958757;0.19382564743619338
195;0.40240274797344844;0.21685215355014872
200;0.43363011686062064;0.2186317776216214
205;0.4951718337647791;0.16938422342895287
210;0.4924158824904376;0.21336167341547418
215;0.4254265141958288;0.23165728584775813
220;0.5437588480206483;0.207003649395527
225;0.4058732044038033;0.22344642480631263
230;0.43052844851287414;0.217731993497403
235;0.36795873749469965;0.29263202931290955
240;0.36746726557387044;0.3086344760600312
245;0.4227607052734599;0.18809366690546878
250;0.45521511807000836;0.20606034442745177
255;0.4120106549076864;0.22155664980408754
260;0.45324174923942157;0.21697943542230858
265;0.38576672690753544;0.26087925495778735
270;0.44271893293059983;0.19989197867683037
275;0.3986465561462589;0.20841061773773398
280;0.4024142540870451;0.24926123032796565
285;0.4140485233882628;0.22284005518819797
290;0.45492421161527097;0.22209280058767095
295;0.4262713915521292;0.2071499016764354
300;0.4841198139828652;0.21398266454458534
