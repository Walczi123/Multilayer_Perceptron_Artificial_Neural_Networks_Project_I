Train dataset: data/regression/data.cube.train.1000.csv
Test dataset: data/regression/data.cube.test.1000.csv
Layers: [1, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.46239763101454606;0.21373963819218778
10;0.4551586083962609;0.2331039054826977
15;0.44273446443297415;0.24488485847589778
20;0.4469873537326148;0.21841769163898542
25;0.450972839552031;0.22764648255196965
30;0.3900414152714265;0.23608454792388145
35;0.38067033805262684;0.2798045096050461
40;0.4397513475145923;0.21641000436439364
45;0.5676549100419821;0.23437686656105391
50;0.5063674233421651;0.2760111859524332
55;0.3883581226960239;0.20237010316033435
60;0.45923311774247133;0.26265054369973617
65;0.4213268035347796;0.2411954162996183
70;0.4381490788329041;0.2525241742308927
75;0.467519847544345;0.24130573672050243
80;0.5188061583081476;0.22937541041459508
85;0.4038367599371067;0.2466256900916668
90;0.4175717182173241;0.2221742198436274
95;0.4074747376173872;0.25948732216555387
100;0.45764877563286477;0.2467427594191426
105;0.4205126351901881;0.2472654781190511
110;0.4102093911063969;0.3212185832446808
115;0.4030501542602934;0.23817141085149351
120;0.5328100460238163;0.24645469247703927
125;0.47738383604474827;0.2250401392664111
130;0.49469653504088396;0.26530699728860085
135;0.44947937873217136;0.2325832694243049
140;0.5360450201606549;0.2817516348819014
145;0.4041319418854664;0.24596669844520946
150;0.4776654092384501;0.21926802114960647
155;0.4254224501204598;0.21097444122387454
160;0.4188514646296851;0.2544615627234777
165;0.40301515933494236;0.23759274357698285
170;0.3654498500151786;0.3182006705858042
175;0.4032261207779284;0.333891138638428
180;0.39623074880539394;0.20191985890489816
185;0.48516129594895413;0.24997354848452868
190;0.43108824981293914;0.26835378583845154
195;0.48390037291197396;0.23999503921698045
200;0.509029189389925;0.22498493262622202
205;0.4169826983366301;0.2707420551668683
210;0.41377294388324704;0.22003892272770903
215;0.4554818512891364;0.26201269344413597
220;0.46122786032808366;0.2551301478821909
225;0.5608632303011384;0.2819920381289255
230;0.42987697983998363;0.2975243267573853
235;0.41474673368658127;0.18779969637927918
240;0.6336340153218514;0.26245478165136793
245;0.42157586604464237;0.22654029039416282
250;0.45781191627909273;0.30069759702595084
255;0.42856330605116566;0.23869252003920688
260;0.39626484856274213;0.23682939868560243
265;0.47324778188743194;0.223892555652471
270;0.4164359209249816;0.18787568578068728
275;0.488610409258206;0.24066425856997883
280;0.39065610278897894;0.2722644874903316
285;0.4009344142715296;0.290474447006981
290;0.43989392498596047;0.2701349019835937
295;0.39139604758283675;0.24288306541491672
300;0.4821545979592905;0.24295977132852334
