Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 128, 64, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.31906726702149035;0.11402905761099463
10;0.3115627946495445;0.12713968557388852
15;0.30998117844106726;0.13244477161829413
20;0.25937075222891126;0.1295038829460262
25;0.4368880337796228;0.13842043396999096
30;0.2935829723020612;0.12744114530094885
35;0.32935509005147184;0.1334270661219167
40;0.3093579535299175;0.13810820290924872
45;0.34339871596404775;0.1290210757891012
50;0.33369103858251453;0.13503165312808735
55;0.35121283985857393;0.127310678648084
60;0.30011881651042216;0.13363477708650842
65;0.32741720601421126;0.13098089344955655
70;0.30953302549789014;0.13906297506830392
75;0.35055973711640515;0.13431981284977118
80;0.3087413147419795;0.12958823781884074
85;0.3220821364983145;0.13000071480981906
90;0.27477911158358986;0.14456549259889867
95;0.37590910740632183;0.15040177702118362
100;0.3390795326094461;0.12913263045536696
105;0.4104493722916599;0.13022890471157456
110;0.33016130544769823;0.14007962869548743
115;0.3029881865102379;0.1342762551094648
120;0.3242667628418259;0.139872463358848
125;0.3534255490920798;0.1335336116853672
130;0.4252338965646049;0.14219474622338954
135;0.3640080608467432;0.13723625986476606
140;0.3532277502523727;0.12958245508484922
145;0.4463093203154107;0.13123475670139673
150;0.2646699611410301;0.13889686044198638
155;0.29319975408500015;0.12974752651539873
160;0.4129099614608452;0.13375003981485753
165;0.32368674144037674;0.1280070065348054
170;0.25486205718273486;0.21232334967345046
175;0.30382759367474577;0.1351257414766213
180;0.31936119855311174;0.13481627445216152
185;0.2772045674710516;0.14662229046979103
190;0.2996479232464613;0.13329414870209735
195;0.3184774726502147;0.13445972305601886
200;0.3459340777361228;0.13663490068358666
205;0.33529440869072413;0.13468868515267918
210;0.3060060027232167;0.13754477250998143
215;0.30523337972035003;0.13122563365499787
220;0.3354704096206209;0.13836613763365951
225;0.27094490053033377;0.14760192465647506
230;0.26480909633679406;0.14920165354550344
235;0.3365288248714654;0.13727076809117358
240;0.3414718928890688;0.13256244047011886
245;0.36220522897872903;0.139718967966231
250;0.309875685457217;0.13523106862903173
255;0.2897327810464339;0.13278630287012538
260;0.26106275581478505;0.14413928782348534
265;0.282146004339717;0.14232183327934742
270;0.33964160669438626;0.13548745745050497
275;0.34198507711736337;0.1313988607669846
280;0.41172544677049994;0.1370348620201039
285;0.3031701054601891;0.13857357672390797
290;0.32055006569535044;0.1322188274810888
295;0.33304904789347406;0.1312164798992614
300;0.3044071150991402;0.13273219261289926
