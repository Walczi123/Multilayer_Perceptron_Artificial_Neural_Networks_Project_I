Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 8, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.36738521234884175;0.13788705860426936
10;0.2360958659713546;0.12793208330280229
15;0.39643597504237016;0.13463178853468308
20;0.27228674715250967;0.11740682383597492
25;0.39077343761659716;0.13746110316276894
30;0.3092414196976034;0.11336566066693468
35;0.39624351377439676;0.13998543643304365
40;0.2872448201176307;0.11057825135627869
45;0.2907606266079912;0.09862792749812826
50;0.27244729031979803;0.13804152829268684
55;0.31013978201077275;0.12238159812547542
60;0.3417757153739364;0.12382414679942799
65;0.3066398191815596;0.11942847275591462
70;0.29254231244723905;0.11748530354854012
75;0.310493955059303;0.1183867448629826
80;0.2989425001972028;0.10564174895897294
85;0.36276702703812286;0.14392700462612285
90;0.3176068213795909;0.12461865288560467
95;0.2748574082064933;0.10259665405259386
100;0.3191752341542761;0.13291849497397804
105;0.26619620746150374;0.11106839476588379
110;0.3030997665365731;0.12045065942133618
115;0.40077277564647107;0.13358220741723334
120;0.2681798594975081;0.11191323012103935
125;0.29742244963045683;0.1292610334272831
130;0.3328373704355385;0.12181661408219426
135;0.3075760739522178;0.12389275163595806
140;0.33069912845583604;0.13134007126012243
145;0.40045528983986717;0.14958143449741876
150;0.32894218317910046;0.13095574379977434
155;0.29681305210720804;0.11586897693971332
160;0.3100628728858234;0.12462657286446403
165;0.35234784518699863;0.12172889759406
170;0.3090371254439756;0.1224293101631408
175;0.3208145564133107;0.12024348285144208
180;0.29725962682345247;0.11965884872443333
185;0.30581740201367535;0.11223506729600227
190;0.30391739783136895;0.11623502374284077
195;0.34078175638182007;0.12482362035135001
200;0.3183785542834629;0.12719555903592406
205;0.38331000826176104;0.14452409182667347
210;0.28863145364680615;0.11035375735187623
215;0.3252794871184854;0.13128508238662598
220;0.322739238983167;0.12812987933368733
225;0.28749036959153657;0.11249764423967899
230;0.3471724300670994;0.12572487688009198
235;0.36745343001672987;0.13239101632056796
240;0.3543273083404325;0.14183582338636577
245;0.29377320861926565;0.1166341417716214
250;0.34212195897956643;0.13491022307110132
255;0.2748519287291074;0.11399243804943518
260;0.3021308613866532;0.12084047087087911
265;0.34125763830550254;0.12521740377109447
270;0.26638032969022046;0.11476905226276458
275;0.32503686546794924;0.12691954418086976
280;0.31509234052613133;0.12637483241102931
285;0.29813022681227735;0.12384358107594233
290;0.286252753649097;0.1120037947305171
295;0.28299362121956884;0.12194319471650276
300;0.28817683991646176;0.11652575748550499
