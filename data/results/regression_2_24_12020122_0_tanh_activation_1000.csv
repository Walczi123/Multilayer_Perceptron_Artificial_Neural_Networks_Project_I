Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 16, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.33095816910666853;0.11268776486203062
10;0.2947628556750814;0.10894730092135073
15;0.2963821694572668;0.126313516779828
20;0.2422164069145163;0.12024522659229125
25;0.3962032926382655;0.1134558595775034
30;0.30461743360406285;0.10734291269398262
35;0.29219767884275644;0.11116511049604341
40;0.2995264674971167;0.09832309070145902
45;0.3520593564531423;0.12007290331617107
50;0.3333998979856254;0.12479083051200422
55;0.3309741154570138;0.10895623683053413
60;0.3110998728479773;0.10464735454376174
65;0.3255788734004797;0.10325019236532204
70;0.3273240064445302;0.1106610278663263
75;0.33013889469574575;0.11134323019121571
80;0.3004423061885308;0.1059179095238953
85;0.30133703701254233;0.09712415622881247
90;0.28530285881677075;0.11165860575490698
95;0.3496102834376696;0.12870536305459626
100;0.38704391010120265;0.10511670866039063
105;0.5118255550956624;0.11899593086277922
110;0.3079368166413505;0.1189199571604678
115;0.2766372450456386;0.12787902102287727
120;0.33795102814947264;0.10362988354109114
125;0.3088841399877635;0.10998964098252301
130;0.4048108454517578;0.13622952568247124
135;0.3374412081527706;0.11297397497678818
140;0.3589447081112127;0.11659046551061575
145;0.3964023637928178;0.12103548421155491
150;0.25394767163000553;0.12848437339693025
155;0.25541839964038954;0.11275018071591646
160;0.5140763831413585;0.11938762450323165
165;0.33721782232083747;0.09979228486203393
170;0.2346038814375934;0.13386140800766363
175;0.2672459891412008;0.10387178257028784
180;0.3816648763169838;0.11661958970590407
185;0.24635343774908217;0.11915122248257047
190;0.2663816167410806;0.10231171935143414
195;0.314909903046788;0.10674719817567278
200;0.36167371361551137;0.10746677795670678
205;0.35132291684675016;0.10729551791135918
210;0.36017682950907487;0.11261481593251586
215;0.34187365584982965;0.10046981531978401
220;0.3128381260588182;0.1121418063653815
225;0.26567273649147966;0.11830866467571245
230;0.2706690461375692;0.12756862487422435
235;0.36386710587202903;0.10797173704386218
240;0.31206084883756413;0.10925860503970387
245;0.4332340844686064;0.12157935938199387
250;0.2493215553233588;0.11083868371765217
255;0.24303732038351483;0.11000124535686175
260;0.23293121095691016;0.14672649070288396
265;0.30948109847759253;0.11320639166683698
270;0.3037412269596574;0.11538510098541907
275;0.3331095876213632;0.11389505287959684
280;0.3884783509783847;0.12341330639290742
285;0.3096470829384618;0.11773259236723439
290;0.3117012913356913;0.10236173618599528
295;0.24045077844027649;0.1177813261045794
300;0.32657795593353156;0.09841401938612614
