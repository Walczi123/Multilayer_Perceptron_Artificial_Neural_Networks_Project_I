Train dataset: data/regression/data.cube.train.1000.csv
Test dataset: data/regression/data.cube.test.1000.csv
Layers: [1, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.45427423095594033;0.19357099464272146
10;0.4656109886977167;0.19349016589376666
15;0.43951892078466537;0.19658673569884638
20;0.43550116338394995;0.20181181228986741
25;0.44475778190419285;0.1961721090845356
30;0.36653194222013447;0.284707889908601
35;0.3692734385889731;0.254260268016253
40;0.41839760776684015;0.20234497175116067
45;0.4924392976585599;0.19562793483601978
50;0.49867202651095915;0.19912019421439545
55;0.3575238735465284;0.2705511754703516
60;0.43894565777655736;0.2030547720522091
65;0.40701119961582594;0.21273349988627108
70;0.43051338759992896;0.2011901052779826
75;0.46136537252838494;0.19925024153121568
80;0.4858080446137262;0.19711287085150575
85;0.3805749134643691;0.22879401933062649
90;0.39538544847163165;0.2187249421716762
95;0.3893836217717659;0.2238158266089512
100;0.4454891730567387;0.20493224997693904
105;0.42052986363427913;0.20148573229733946
110;0.41027831351926775;0.22434035410763656
115;0.37987237505296206;0.22568044590900566
120;0.5070348657995987;0.19096830598173511
125;0.45437318993100223;0.1921946970667681
130;0.47093075245544314;0.20104170748621625
135;0.42512826821266503;0.20152732499875392
140;0.5443935369102488;0.20446611521893196
145;0.3756348876746627;0.23586183266014993
150;0.4677638853178231;0.19496945744432415
155;0.39584115320182206;0.21453906640550893
160;0.3979414512328313;0.207003157201511
165;0.3960543162642866;0.21161710858517035
170;0.35487626592464117;0.3341500249629575
175;0.40089375368440106;0.23592050880206641
180;0.3625221175847428;0.25037489080017356
185;0.4781160830381577;0.2083307275542439
190;0.43267609884761143;0.2086838248233259
195;0.4819654612929673;0.19702113666270968
200;0.48946045473104094;0.19748978866836112
205;0.41029414332609243;0.21471783764537453
210;0.3927023061460139;0.21067161423074535
215;0.4627292926619993;0.195844333878264
220;0.4385857946914455;0.20269544042180943
225;0.5454430172413867;0.21529232196973241
230;0.4139838513215581;0.22379921570876643
235;0.36088443409977844;0.25196649829530027
240;0.608423760687853;0.19562605664817428
245;0.39959613029883206;0.21088682827614372
250;0.4833239802025696;0.19811832736390333
255;0.39882031181537253;0.21535035509545095
260;0.3804778232144739;0.22985107139321145
265;0.4709589434834691;0.1954619808049001
270;0.36170970003563935;0.2503550986073129
275;0.48537243142206926;0.19491479557203667
280;0.372550605401675;0.236719058505423
285;0.3951662125436675;0.21586577630014658
290;0.4499390765669179;0.20421851411895098
295;0.36909934295205765;0.23952256699525
300;0.4712953367883673;0.20520760795440332
