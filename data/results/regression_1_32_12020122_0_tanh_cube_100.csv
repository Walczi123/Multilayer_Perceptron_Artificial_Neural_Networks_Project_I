Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 32, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.5507104849752568;0.32333786193710135
10;0.7170865330362336;0.359130964235831
15;0.394336371476559;0.17239824023753506
20;0.39353459911898897;0.19246791206695335
25;0.43555363812515113;0.17784682129749055
30;0.7656741645128918;0.15433484333124597
35;0.6327641808940822;0.15392403692896314
40;0.7841300534488924;0.16394067767123352
45;0.4405043444992559;0.16970194253118906
50;0.37190118579362585;0.21446594926266618
55;0.512077684679666;0.15643166851603038
60;0.39099689139203;0.19531110498397897
65;0.5823909017493885;0.15764880640984574
70;0.38180129255188416;0.19493140257793296
75;0.39604988402928626;0.18099434957174884
80;0.5034179327229342;0.16244804897880055
85;0.5219154356306995;0.15124353264179446
90;0.3640096073198323;0.24973301193907518
95;0.524287899411416;0.16669678296652654
100;0.5245526228625342;0.16074996254660062
105;0.5669324996930607;0.17170872645534233
110;0.41935649388574425;0.18070588740443527
115;0.4122608120585878;0.18693316023197065
120;0.4458812064013228;0.16927349342015635
125;0.37423783387062615;0.47700982963140626
130;0.43410440067729494;0.1757434987816663
135;0.4372711303685472;0.16880531027562917
140;0.3887112709312694;0.23986550891317984
145;0.4688471367934521;0.1578616974340149
150;0.4089672886514579;0.19910201480872805
155;0.42716098071534087;0.20215192822516065
160;0.47910827107809106;0.17765710038217483
165;0.38449288765185713;0.4794752156182668
170;0.5083981691069228;0.17863440346054155
175;0.4023672596609396;0.1921991108809327
180;0.6813624961190541;0.1567999749430818
185;0.6538478076797927;0.1664355585812861
190;0.41795033184747715;0.1715032876663047
195;0.3889916434129382;0.17384736163748862
200;0.4082256328459409;0.1779176805563622
205;0.455053404531119;0.1592881768263351
210;0.5539913200960933;0.16513560732991472
215;0.5347639804630268;0.17232093787136907
220;0.6748143146125462;0.16555927987642272
225;0.36251549909434;0.24675986787518464
230;0.4122611272219582;0.17891543173037694
235;0.3620088318403018;0.33132249976960204
240;0.35660509201706514;0.30906530792732
245;0.42469774070371846;0.1588018035322736
250;0.5970988635251807;0.1667361623663759
255;0.4015266082513086;0.19925207423831295
260;0.5769136240315214;0.1686108968910186
265;0.3621290720368418;0.2802879112003317
270;0.39309324256735817;0.2100686231864949
275;0.36780845262110606;0.22571336245281104
280;0.39745570631537896;0.2093522802341499
285;0.43084031939790257;0.18255390947567088
290;0.5108012585600531;0.17594373350830556
295;0.38643250113826066;0.20936748278733266
300;0.4919042612210935;0.16793657792102237
