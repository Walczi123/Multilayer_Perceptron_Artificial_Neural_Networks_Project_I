Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 8, 4, 2, 1]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.0009501391890450993;0.03390463977527133
10;0.00099339563946965;0.0341944124995808
15;0.0009565852061241143;0.033865315474854184
20;0.0009399155917032468;0.033743769614450095
25;0.0009470476115494745;0.03390740329029862
30;0.0009197539402636962;0.033666842165897035
35;0.0008984247566150095;0.03352202953962629
40;0.0008941216238622014;0.03351391994073929
45;0.0009051366495665089;0.0336791620386279
50;0.000877127606176063;0.033454498117522116
55;0.0009154737844730686;0.03392209446387877
60;0.0008804150753027406;0.033594949685089665
65;0.0008916410676249718;0.033768309543599155
70;0.0008925740033913372;0.03383099373955226
75;0.0008515447282509787;0.033410639241186715
80;0.0008572499904757999;0.03352121661542433
85;0.0008663866051328478;0.0336836739283528
90;0.0008799355842114674;0.03391698219028977
95;0.0008556614494794906;0.033683272294436475
100;0.0008685670328859366;0.03387283073197429
105;0.0008292743723054272;0.03347114128803177
110;0.0008540719927876836;0.033813720519213844
115;0.0008204514435455671;0.03347393266281635
120;0.0008233980487202947;0.03354077931880145
125;0.0008192862565634274;0.033537840858368036
130;0.0008149945213807761;0.033541490444763726
135;0.0008286116575136136;0.03375071746211484
140;0.0008137513077690778;0.03360889500866062
145;0.000803598086033257;0.03354647001216174
150;0.000810404294282061;0.033665392191097245
155;0.0007681170566326173;0.033192524510007246
160;0.0007770948942087793;0.033346359252762615
165;0.0007800846490880738;0.03342955903735791
170;0.0007619517760448928;0.03324576167469641
175;0.0007652894195810966;0.03333550533858812
180;0.0007485068104272294;0.033155532474053784
185;0.0007836997674721159;0.03364499935099339
190;0.0007677312385011638;0.033485688319632896
195;0.0007489562981204235;0.033281404006358345
200;0.0007715345484220644;0.03359660357812795
205;0.0007624304526587815;0.03353718942135746
210;0.0007447908831244816;0.03336145564453412
215;0.0007448047673048764;0.033409312278999334
220;0.0007321411547112258;0.03327740014142993
225;0.0007323097413962934;0.03330504089900022
230;0.0007239694622782768;0.03323646948453443
235;0.0007384147476059308;0.03346890016478952
240;0.0007175463604863056;0.03323185959414415
245;0.0007302337174506849;0.03343631361153694
250;0.0007263558259105828;0.03342139940410395
255;0.0006793592744524478;0.032805151424977516
260;0.0006847562241225831;0.03292211324439407
265;0.0007149724317690252;0.03338780723891824
270;0.0007174734946844368;0.03346848866948938
275;0.0007143237608971108;0.03343241853343922
280;0.0006982709705572409;0.033266096597303986
285;0.0006952373522956812;0.033265066729314285
290;0.0006682081735602366;0.032908818888551294
295;0.000679626258563338;0.0331052607790096
300;0.0006872001707933402;0.033244068164658094
