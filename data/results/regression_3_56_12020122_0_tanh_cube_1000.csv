Train dataset: data/regression/data.cube.train.1000.csv
Test dataset: data/regression/data.cube.test.1000.csv
Layers: [1, 32, 16, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.43902980002751385;0.19873252655910478
10;0.449678741623032;0.2070673063945389
15;0.4385420991100193;0.22483173677626023
20;0.5065321363860382;0.19475433003785103
25;0.49391155521576097;0.19794430910002497
30;0.3959061433754155;0.24835510932813096
35;0.38157228853993014;0.2909444953793349
40;0.442287161886608;0.21118994623171206
45;0.5209328540086363;0.22727692457884777
50;0.49938437245104284;0.2417924987743289
55;0.45596388005499905;0.159712659045107
60;0.4512593589375093;0.2224247673769961
65;0.4383217615561909;0.21967581719555737
70;0.44528646331590516;0.22401262270799968
75;0.4512502854520565;0.2150412944595731
80;0.5034537158512998;0.2112944001737607
85;0.4356367856347496;0.20187880889283197
90;0.4132045900095736;0.20126720634953163
95;0.43174983840843745;0.21425562243745702
100;0.4418163422454607;0.23608843021956621
105;0.4334983322573602;0.22466130621158087
110;0.38455221561944364;0.337561398654092
115;0.4065616716895269;0.21104553785962865
120;0.4270863444628212;0.23638243329655245
125;0.43682860179346444;0.20325175475533636
130;0.4737259559236539;0.25238441096608716
135;0.4539924921564393;0.21042780012639267
140;0.5420695613979039;0.24315591244361737
145;0.4289911387582873;0.2089042070639127
150;0.5565666168059187;0.1820757782745681
155;0.449332978578594;0.18561952782121807
160;0.4134142493289776;0.23286745132896702
165;0.4098821304048559;0.2107869516748488
170;0.3657769500434357;0.29878218395614425
175;0.39926999346685316;0.3134969310206134
180;0.4246094308809677;0.15944496938177008
185;0.4858963014168107;0.23848943450891816
190;0.4296936620503401;0.24484805892458597
195;0.5251280750731259;0.20529749244951914
200;0.4894986630641925;0.1981753226697674
205;0.4611056730460334;0.22137395017581504
210;0.43096602972207393;0.20269172975143535
215;0.4615864712821352;0.22156116613140345
220;0.4451731510750689;0.22851562174031856
225;0.5784416776249209;0.2498392840557771
230;0.3819348585779254;0.36002881991093
235;0.4948311004161154;0.15607910841334316
240;0.5596145003195787;0.23286691162713685
245;0.4237964477805605;0.2101747854356123
250;0.46461312468805915;0.23902432972891144
255;0.41669147807820667;0.21819653118138546
260;0.4340181782776867;0.20656964273159098
265;0.5442971730592295;0.19170364910455717
270;0.4960646086394075;0.15955616953072232
275;0.4785878140745797;0.21916770851896536
280;0.3970325614960084;0.2546198062708909
285;0.39088533343867304;0.2532557438408525
290;0.44499801548995316;0.22785864646622103
295;0.3959257071146947;0.24298821441998442
300;0.516556802378362;0.21434898862790985
