Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 64, 32, 16, 8, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.31647829676721084;0.11774655610086819
10;0.3104194973636399;0.14513059058932118
15;0.3304437965057032;0.13397317207556622
20;0.28023774352238634;0.12792993382163434
25;0.41954810834125467;0.14921622607302104
30;0.29032676192829604;0.13303537387426564
35;0.3260929617164313;0.13306900826535167
40;0.30503414584473304;0.13619952883949837
45;0.33353284406019773;0.12965334373435475
50;0.3380959138058107;0.14428817882708503
55;0.3422228048983127;0.13052287851836383
60;0.31024348471144586;0.13576118113003666
65;0.3286679175277018;0.1348555933477344
70;0.3115093383954192;0.13886869476974284
75;0.33808790618787127;0.13488065512052125
80;0.30223281887991993;0.1344184505113213
85;0.31632066856646907;0.13140004962407226
90;0.2821851817890155;0.1472303437607707
95;0.38472541905219976;0.15200174605111577
100;0.3431290924483231;0.13134625542526462
105;0.37919156172037644;0.1307639203900488
110;0.32184161395266037;0.13803128230597153
115;0.30587196767706915;0.13295121693229098
120;0.33537154894400145;0.13680406247332863
125;0.3535842123012755;0.13663888775879418
130;0.41074504336749385;0.1352442943872081
135;0.34673699937959596;0.13192999181852036
140;0.34030247954401865;0.1316798502869538
145;0.4071432666263627;0.13534256552958482
150;0.27527883613823834;0.13697513604599404
155;0.2815892423437889;0.12904883501079487
160;0.39055733641888724;0.13775155200686354
165;0.31845010038190347;0.13005176055354137
170;0.24699138230447895;0.1942455180757098
175;0.3037894936580893;0.13187026792308112
180;0.3101556283681797;0.13435724034668645
185;0.27482331628954265;0.1483561957182381
190;0.29759317555013814;0.1330529652807193
195;0.3133771961862015;0.13188366765671614
200;0.3402478876080515;0.13668519222769546
205;0.3295437583330243;0.1357551404243754
210;0.31528709284163975;0.14098241444765877
215;0.31191914906389634;0.13397245921282766
220;0.33826637503411117;0.14235563182877287
225;0.2700962633728275;0.15625789714406368
230;0.2732469975454177;0.13975075151033026
235;0.3307252616460094;0.13862034932900807
240;0.33049132488583904;0.1327519380161234
245;0.3532777003577397;0.14292198026529476
250;0.31766262257832556;0.13759072506330386
255;0.28726162980479164;0.13003181506736355
260;0.2738755852673888;0.13653025405129685
265;0.31911023081897616;0.13376330237507017
270;0.33928227171013264;0.13730254395435548
275;0.3439179122979703;0.13166394847011273
280;0.3914392828319772;0.13396437472167794
285;0.3102963439107947;0.14001140610218063
290;0.28207111632929127;0.13897798557028346
295;0.3358916242214753;0.13105573057194556
300;0.31434481414935445;0.13415837435780567
