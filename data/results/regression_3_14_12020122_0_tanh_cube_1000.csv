Train dataset: data/regression/data.cube.train.1000.csv
Test dataset: data/regression/data.cube.test.1000.csv
Layers: [1, 8, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.4570181618810311;0.22496379128025232
10;0.4468277850610793;0.24424244257667405
15;0.43027311423413556;0.26124560258000395
20;0.45452215670526736;0.22159979565516918
25;0.44647801431961964;0.2394912818337386
30;0.4030164959757522;0.23279179656994206
35;0.3802583269249028;0.2992464016348203
40;0.4434002655581;0.22869124722275205
45;0.5642942958291305;0.24750854462357785
50;0.5019128968301716;0.27871209904623084
55;0.43379301098003187;0.1753748187310009
60;0.4501729864674876;0.2598172009132928
65;0.4420849099033279;0.24476250328993154
70;0.44044484632293873;0.2637905010584109
75;0.45249557773561544;0.23682298395295737
80;0.5133552601669299;0.2727643199840832
85;0.42999284742269633;0.23135970864842542
90;0.4149864459582964;0.22431205444733257
95;0.4383418878992126;0.23857535965000626
100;0.48074212568942354;0.26866383265098437
105;0.4376510301912266;0.28522573090506614
110;0.412384928807313;0.2620060444610004
115;0.4503448923575982;0.23356415771588443
120;0.46903202745591793;0.25989070204126796
125;0.4430439523289982;0.25345253607267687
130;0.5201695020677349;0.29681550912721716
135;0.45175831443711134;0.23516425722105122
140;0.5774724155622645;0.26874472654448994
145;0.4557088524754755;0.2399174745614377
150;0.5119692427010494;0.2576931837499915
155;0.5072975720286482;0.2602758869922691
160;0.4131170332665385;0.2692224027086495
165;0.42188579604923704;0.21421730634238526
170;0.3884571109542365;0.26734501073685135
175;0.42310501302231596;0.30061333891985814
180;0.4555715063045338;0.2179056682277811
185;0.4939391704312337;0.2747307161019371
190;0.4360163675632252;0.2523104608273055
195;0.4858156992007389;0.27860437243685693
200;0.44191406648802706;0.243341820500473
205;0.4356491107992947;0.3355759735551591
210;0.46860481477263916;0.250038142287109
215;0.451920222387818;0.24928963530539575
220;0.4149714711713352;0.2867666517092708
225;0.5688103992927767;0.2633488151920749
230;0.41833014571349086;0.3109647193579342
235;0.4863446731069691;0.21911040885570837
240;0.6232062697127262;0.2605113812629723
245;0.43562007436830896;0.2384043443995555
250;0.4722687836887086;0.2343716444692992
255;0.4420767103942118;0.2568112215895047
260;0.4606983180577149;0.2694079248786235
265;0.5011396400852609;0.26189279627713363
270;0.4746981233225921;0.24429968759923906
275;0.44570753396124935;0.24472734051783698
280;0.41436860130533837;0.2545127653739769
285;0.41872653356513917;0.25411785568095685
290;0.47974493742306523;0.2629286840945336
295;0.44877421132403167;0.2690604593698994
300;0.4593285555152795;0.26790610173904067
