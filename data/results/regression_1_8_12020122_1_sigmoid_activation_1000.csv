Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 8, 1]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: sigmoid
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.00041966387010205683;0.019103651268983824
10;0.00039355263921986945;0.02013095160909096
15;0.00038827736609410145;0.021129252134654643
20;0.000421935623653544;0.023175697820570516
25;0.0003820028061161529;0.02333883274996263
30;0.00042549330604338046;0.023931511385485903
35;0.0004541083006636747;0.025943909587157966
40;0.0004086896847707819;0.025188095209722292
45;0.0004347839678842648;0.025935404756505992
50;0.0004237457248688698;0.026055517987836146
55;0.00045288277300568327;0.026781432380209177
60;0.0004413689785108587;0.026699925410230366
65;0.00046906000267343015;0.02732561186231978
70;0.000490239535069111;0.027749711676971354
75;0.0004936063914457363;0.028019032635515292
80;0.0004691423078126549;0.02766040345603083
85;0.000483814989946527;0.02805626210374127
90;0.0004952594094238336;0.028309930750783533
95;0.0005069630345172095;0.028634121152367832
100;0.0005144971233278323;0.02881087426415398
105;0.0005028922620359656;0.02870815046025315
110;0.0005135467444759862;0.028949639881215364
115;0.0005059031662231567;0.028876042051765003
120;0.0005038142585587198;0.028877327992749426
125;0.0005213907440009714;0.029231596148323047
130;0.0005112308545726402;0.02912233308812093
135;0.0005210406989811425;0.029332960381874842
140;0.0004989895333550769;0.02900815933248472
145;0.000527380361680964;0.02952788896625613
150;0.0005112862723732482;0.029299243432805763
155;0.0004979380726815245;0.029118265772394614
160;0.0005162687880896239;0.029471097619758354
165;0.000506248268413843;0.02933846232345274
170;0.0004930535839927268;0.029151882395733714
175;0.0005029188920170739;0.029356815232200607
180;0.0005045968425293001;0.029421911800488393
185;0.0005061456642152157;0.029489960398233234
190;0.0005084995933611619;0.029571650301472786
195;0.0005070907773261847;0.029581429296251974
200;0.0004968367325904541;0.029437145768760643
205;0.0004960687201052831;0.029462017849189066
210;0.0004911158489979924;0.029399699300164485
215;0.0004886858962083214;0.029392706776376776
220;0.0004968608466465624;0.029567432786541104
225;0.0004951725966511045;0.029577604667171575
230;0.0004777097491853144;0.029295755590614456
235;0.0004809609648036189;0.029391538499923608
240;0.0004706350049393487;0.029232377873818004
245;0.00048472179308701395;0.029510286285204353
250;0.00048164035181553203;0.02948646223404065
255;0.0004503308983127691;0.02894012232227913
260;0.0004543184687369204;0.029041900312900843
265;0.0004785045851130296;0.029517365432508488
270;0.0004735246011178463;0.029458731104944357
275;0.0004766384325787008;0.029542529209723008
280;0.0004652021473192647;0.029361596951204535
285;0.0004616227984596233;0.029324153343945576
290;0.0004492413457679124;0.029119664501053452
295;0.00045130818555653214;0.029181262836975103
300;0.00045698732241990995;0.0293138850588033
