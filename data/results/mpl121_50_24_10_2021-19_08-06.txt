Train dataset: data/regression/data.cube.train.100.csv
Test dataset: data/regression/data.cube.test.100.csv
Layers: [1, 2, 1] with biases
Seed: 12020122
Epochs	Loss
2	[0.0126613  0.01256264 0.01246465 ... 0.00047509 0.00048529 0.00049563]
4	[0.01009057 0.01000251 0.0099151  ... 0.00114699 0.0011628  0.00117878]
6	[0.00965487 0.00956874 0.00948325 ... 0.00130031 0.00131714 0.00133415]
8	[0.00957064 0.00948488 0.00939976 ... 0.00133148 0.00134851 0.00136571]
10	[0.00955525 0.00946957 0.00938452 ... 0.00133723 0.00135429 0.00137153]
12	[0.00955262 0.00946694 0.00938191 ... 0.00133821 0.00135528 0.00137253]
14	[0.00955219 0.00946651 0.00938148 ... 0.00133837 0.00135545 0.0013727 ]
16	[0.00955212 0.00946644 0.00938141 ... 0.0013384  0.00135547 0.00137272]
18	[0.00955211 0.00946643 0.0093814  ... 0.0013384  0.00135548 0.00137273]
20	[0.0095521  0.00946643 0.0093814  ... 0.00133841 0.00135548 0.00137273]
22	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
24	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
26	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
28	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
30	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
32	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
34	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
36	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
38	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
40	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
42	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
44	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
46	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
48	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
50	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
52	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
54	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
56	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
58	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
60	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
62	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
64	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
66	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
68	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
70	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
72	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
74	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
76	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
78	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
80	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
82	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
84	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
86	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
88	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
90	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
92	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
94	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
96	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
98	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
100	[0.0095521  0.00946643 0.00938139 ... 0.00133841 0.00135548 0.00137273]
