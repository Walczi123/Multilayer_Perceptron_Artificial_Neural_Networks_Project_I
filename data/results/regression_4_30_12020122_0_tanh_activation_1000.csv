Train dataset: data/regression/data.activation.train.1000.csv
Test dataset: data/regression/data.activation.test.1000.csv
Layers: [1, 16, 8, 4, 2, 1]
Test dataset: 0
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.3076307873740179;0.11897864536978683
10;0.31874936842565904;0.14082040092785264
15;0.3190164722470991;0.13565839621874698
20;0.28658894677107805;0.12353517225601544
25;0.3846823745219406;0.1545751720716889
30;0.2881827537107969;0.13283604294488852
35;0.31456743955281535;0.13097617960630198
40;0.3198054231958236;0.14216100342410443
45;0.3157205302445171;0.13331092921943927
50;0.33317621953293913;0.1497527481471986
55;0.31935091654600833;0.13465908136514948
60;0.32044999379747224;0.13844335906482463
65;0.31754024480982646;0.13886826208291972
70;0.3097905512894065;0.13963922500962836
75;0.30686556892356553;0.14156015984656367
80;0.3061338285670436;0.1354000719241483
85;0.31349245899382083;0.13042304894009266
90;0.3067605575884856;0.151197830389207
95;0.40368307022129224;0.1622760090085453
100;0.33086018820581814;0.1359214228904072
105;0.34051962666172214;0.1369801010313143
110;0.3206224900576905;0.13873755297781168
115;0.3177806469782941;0.1325427855511308
120;0.33045690045124154;0.13980499206349478
125;0.33816307816003616;0.14202474486769637
130;0.3678079602466642;0.14126834331287394
135;0.33724577755967;0.13777149215294981
140;0.3225574248021263;0.13319170166617078
145;0.3549635963172898;0.14257420300305693
150;0.2862587616325103;0.13623052443309672
155;0.3137276914292326;0.12707208076060125
160;0.35691838868716425;0.1399765309187068
165;0.3060881521439163;0.1313572967920856
170;0.26805484905483706;0.17064075779595084
175;0.29366350889568066;0.13201052828138238
180;0.3004087917240441;0.13698708036605556
185;0.2889183533576415;0.14276823801770921
190;0.3075153316700567;0.1336088142795159
195;0.30344092596972594;0.13701952559091268
200;0.3306007311649121;0.14132749154542842
205;0.30907189377748456;0.13700683262751917
210;0.3153223011040633;0.14350289113997328
215;0.3202654820816784;0.1376569410346734
220;0.3378394468001822;0.14711565799374737
225;0.2759654497999211;0.15599164269102342
230;0.3055988314946247;0.1432529252507425
235;0.32036344757625057;0.13976732708332418
240;0.3221814847368549;0.1323982081809997
245;0.33665205957319877;0.14692553305688
250;0.33979768542231603;0.13830767795275306
255;0.29465556544174976;0.12997294694015965
260;0.2889932634009657;0.13516285304407194
265;0.3149850922122984;0.1414154295398707
270;0.3377828304939448;0.14293501746628148
275;0.3275355997895436;0.13857053156089333
280;0.3586925991745517;0.14030614132037764
285;0.3188323912388471;0.14054290610542727
290;0.30511716405741135;0.14031167063576927
295;0.3153799686457449;0.13584665028174966
300;0.3229771618759421;0.13760647156262043
