Train dataset: data/classification/data.three_gauss.train.100.csv
Test dataset: data/classification/data.three_gauss.test.100.csv
Layers: [2, 32, 16, 8, 3]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: softmax
Problem type: classification
Epochs: 300
Epochs;Loss (cross_entropy);Loss (hinge);Accuracy
5;-434.2604056993208;0.1;93.0
10;-438.8383047575003;0.05333333333333334;94.0
15;-452.3614493951085;0.08;92.33333333333333
20;-461.90916141953073;0.05333333333333334;93.66666666666667
25;-465.54479605912593;0.08;92.33333333333333
30;-449.75875985966417;0.06;93.33333333333333
35;-429.70960900789794;0.04666666666666667;93.33333333333333
40;-450.5696900425471;0.06666666666666667;93.66666666666667
45;-436.2877311565283;0.07333333333333333;93.33333333333333
50;-462.380293628823;0.06666666666666667;93.33333333333333
55;-448.55591576871797;0.05333333333333334;93.0
60;-450.11210901663327;0.04;93.33333333333333
65;-432.01106532084555;0.05333333333333334;93.33333333333333
70;-458.6133247535264;0.05333333333333334;93.33333333333333
75;-444.67116384109835;0.08;93.33333333333333
80;-472.4377026136315;0.06666666666666667;92.33333333333333
85;-453.76129483960676;0.06666666666666667;92.66666666666666
90;-456.4161003095235;0.05333333333333334;92.66666666666666
95;-468.02970254224726;0.08666666666666667;91.0
100;-450.5696900425471;0.07333333333333333;93.0
105;-447.2738533765428;0.07333333333333333;92.66666666666666
110;-445.07662893253985;0.07333333333333333;93.33333333333333
115;-433.5151426342885;0.08;93.33333333333333
120;-442.99718754086;0.06;93.33333333333333
125;-447.5615354156612;0.06;94.0
130;-442.35615634477244;0.09333333333333334;92.0
135;-447.3916364288658;0.06;93.33333333333333
140;-451.72041819902097;0.09333333333333334;93.0
145;-458.73110780584943;0.04;94.0
150;-444.9588458802168;0.06666666666666667;93.33333333333333
155;-448.72581475551334;0.05333333333333334;93.66666666666667
160;-454.6764568914345;0.06666666666666667;93.66666666666667
165;-462.3146265109723;0.09333333333333334;92.66666666666666
170;-454.50655790463907;0.04;93.33333333333333
175;-463.3611227985013;0.08;92.33333333333333
180;-464.15850179800606;0.04666666666666667;93.66666666666667
185;-458.20785966208484;0.06;93.33333333333333
190;-455.31748808752207;0.05333333333333334;93.0
195;-465.32278113785816;0.04;93.33333333333333
200;-453.4079456826376;0.04;93.66666666666667
205;-452.4792324474315;0.06666666666666667;93.0
210;-459.82972002785084;0.04;93.66666666666667
215;-445.0245129980675;0.06;93.33333333333333
220;-445.7176601286275;0.04;93.33333333333333
225;-455.4352711398451;0.04;93.66666666666667
230;-456.9914643877604;0.07333333333333333;93.0
235;-447.6793184679843;0.04666666666666667;93.33333333333333
240;-459.18868883176333;0.07333333333333333;93.66666666666667
245;-451.7860853168716;0.06;93.33333333333333
250;-460.28730105376474;0.07333333333333333;93.33333333333333
255;-460.05173494911867;0.08;92.66666666666666
260;-444.9067299457445;0.07333333333333333;92.66666666666666
265;-457.4490454136743;0.06;93.66666666666667
270;-464.040718745683;0.06;93.0
275;-465.0872150332121;0.04666666666666667;94.0
280;-426.8713533678075;0.02666666666666667;93.0
285;-471.74455548307157;0.04;93.33333333333333
290;-465.4926801246536;0.04;94.0
295;-448.0847835594258;0.08666666666666667;92.33333333333333
300;-448.2025666117488;0.07333333333333333;93.0
