Train dataset: data/regression/data.activation.train.100.csv
Test dataset: data/regression/data.activation.test.100.csv
Layers: [1, 32, 16, 8, 1]
Test dataset: 1
Seed: 12020122
Learning rate: 0.1
Activation function: tanh
Output function: indentity
Problem type: regression
Epochs: 300
Epochs;Loss (mse);Loss (msle)
5;0.0007073424112820986;0.03847543415939191
10;0.0003113597750323471;0.030831042242074634
15;0.00021017406279009591;0.02799331925927405
20;0.00015565442615266754;0.02563784955197229
25;0.00013746608627036313;0.025475790430968134
30;0.00014680243803497656;0.02623461682569195
35;0.00015107698396427225;0.02633749002964269
40;0.00010075202154326411;0.02356440205703576
45;0.00010944910108875636;0.023911543876011274
50;7.636019193077503e-05;0.021671217634586744
55;8.249709717631061e-05;0.022332794506049677
60;7.113243403325337e-05;0.02173639450724137
65;5.255782735083826e-05;0.019578181006068618
70;6.554382862372418e-05;0.020500114726380494
75;6.287975439649291e-05;0.020879405277423022
80;6.311863749686956e-05;0.021157208904482066
85;5.151772445102144e-05;0.01975294854547436
90;5.073506698232076e-05;0.019775948332047034
95;4.601341738522642e-05;0.018575039058162126
100;4.618745373346826e-05;0.019127393486820427
105;4.442738993171184e-05;0.018504493863139237
110;3.831713130459365e-05;0.018464671674896283
115;3.8238448945520356e-05;0.018258686977698543
120;3.831833737259184e-05;0.018120944147782363
125;3.8428126427566914e-05;0.018469086428278977
130;3.8509525456854873e-05;0.018299630788940584
135;3.0512094362234804e-05;0.01697041338342826
140;4.0752797148624825e-05;0.01841391800700366
145;3.9441556695743076e-05;0.01869880871399833
150;3.261829196217797e-05;0.01762392822581645
155;3.209361221084558e-05;0.017258979236314132
160;3.650233507104203e-05;0.01806741153295296
165;3.096585389182577e-05;0.01781071657917977
170;2.4993259516513754e-05;0.01658652203064353
175;3.5879128810589e-05;0.018326754519377996
180;2.818470308228083e-05;0.017196166388167428
185;2.722921597800337e-05;0.017087616017497507
190;2.6949109266248924e-05;0.01686408954212712
195;3.1762212680861744e-05;0.017200373085871915
200;2.8110600006108613e-05;0.017303741865691107
205;2.9125560478695217e-05;0.017019200476225887
210;3.060158444941427e-05;0.017680423123315286
215;2.3978955776433065e-05;0.016589881024090015
220;3.0647887536848275e-05;0.017272335570271965
225;2.3841009539410627e-05;0.0165015178175118
230;2.7196688357654892e-05;0.01732690650412835
235;2.4316904274526473e-05;0.01671387958335267
240;3.1143044028368784e-05;0.017950317612216913
245;2.4710827406185303e-05;0.01670049556946323
250;2.55703234446329e-05;0.016293078859627844
255;2.5865148157149222e-05;0.017146067351735284
260;2.2590727501982137e-05;0.01646520914765531
265;2.254971327288931e-05;0.016309864553673486
270;2.7383001152999254e-05;0.017246245294423056
275;2.3360402916800845e-05;0.01670514627222844
280;2.5614472280204645e-05;0.017099620095608663
285;2.797641018542835e-05;0.017424870706071564
290;2.456902132095877e-05;0.016862641502979916
295;2.3071756056675796e-05;0.016307669968232113
300;2.5403322453945412e-05;0.01718371670008518
